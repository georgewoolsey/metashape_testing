---
title: "Agisoft Metashape Parameter Testing"
author: "George Woolsey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding){ 
    out_dir <- '../';
    rmarkdown::render(inputFile, encoding = encoding, output_file=file.path(dirname(inputFile), out_dir, 'index.html')) 
  })
---

# Introduction

The objective of this study is to determine the influence of different ~~Agisoft Metashape~~ structure from motion (SfM) software (e.g. Agisoft  Metashap, OpenDroneMap, Pix4D) and processing parameters on processing and forest measurement outcomes. This analysis builds on [Tinkham and Swayze (2021)](https://scholar.google.com/scholar?oi=bibs&hl=en&cluster=11260597505702247290) by including UAS flights from different forests and by also comparing different SfM processing software. UAS flights from the follwing forests were included: the Manitou Experimental Forest on the Pike-San Isabel National Forest (Colorado; "N1"), the Black Hills Experimental Forest on the Black Hills National Forest (South Dakota; "SQ02_04", "SQ09_02", "WA85_02"), and the Lookout Canyon area in the Kaibab National Forest (Arizona; "Kaibab_High", "Kaibab_Low"). 

```{r, include=FALSE, warning=F, message=F}
# knit options
knitr::opts_chunk$set(
  echo = TRUE
  , warning = FALSE
  , message = FALSE
  # , results='hide'
  , fig.width = 10
  , fig.height = 7
)
```

## Load packages

Load all packages used in program.

```{r pkg-load}
# bread-and-butter
library(tidyverse) # the tidyverse
library(viridis) # viridis colors
library(scales) # work with number and plot scales
library(latex2exp)

# visualization
# library(mapview) # interactive html maps
library(kableExtra) # tables
library(patchwork) # combine plots

# # spatial analysis
# library(terra) # raster
# library(sf) # simple features

# modeling
library(brms)
library(tidybayes)
library(ggpubr)
library(rstatix)
library(broom)

# others
library(pdftools) # pdf data extraction
```

# User-Defined Parameters{#udp}

Parameters to be set by the user

```{r parameter-set}
# !!!!!!!!!!!!!!!!!!!!!!! USER-DEFINED PARAMETERS !!!!!!!!!!!!!!!!!!!!!!! # 
###____________________###
### Set directory for outputs ###
###____________________###
# rootdir = "../data"
rootdir = "../data"

###_________________________###
### Set input data directory ###
###_________________________###
# !!!!!!!!!! this is where both pdf and las data reside
# !!!!!!!!!! files should be named in the format {quality}_{depth map filtering}.pdf
# !!!!!!!!!! for example: high_aggressive.pdf; UltraHigh_Disabled.pdf; lowest_MILD.pdf
input_data_dir = "../data/raw_data"

###_________________________###
### list of study site directory names
###_________________________###
# !!!!!!!!!! this is where both pdf and las data reside
# directories matching the names of these study sites will be searched
study_site_list = c(
  "SQ02_04", "SQ09_02", "WA85_02"
  , "Kaibab_High", "Kaibab_Low"
  , "n1"
)

# !!!!!!!!!!!!!!!!!!!!!!! USER-DEFINED PARAMETERS !!!!!!!!!!!!!!!!!!!!!!! #
```

```{r dir-create, include=FALSE, eval=TRUE}
dir.create(paste0(rootdir, "/fits"), showWarnings = F)
```

# Metashape Image Processing

Search for the list of Agisoft Metashape processing report `pdf` files to extract information from in the user-defined `input_data_dir` directory. Parse the list of files to extract processing information and study site information from.

```{r data-load}
# get list of files and directories to read from
pdf_list = list.files(normalizePath(input_data_dir), pattern = ".*\\.(pdf)$", full.names = T, recursive = T)
# set up data.frame for processing
pdf_list_df = dplyr::tibble(
    file_full_path = pdf_list
  ) %>% 
  dplyr::mutate(
    study_site = file_full_path %>% 
      stringr::word(-1, sep = fixed(normalizePath(input_data_dir))) %>% 
      toupper() %>% 
      stringr::str_extract(pattern = paste(toupper(study_site_list),collapse = "|"))
    , quality_filtering = file_full_path %>% 
      stringr::word(-1, sep = fixed("/")) %>% 
      stringr::word(1, sep = fixed(".")) %>% 
      toupper()
    , metashape_quality = quality_filtering %>% stringr::word(1, sep = fixed("_"))
    , metashape_depthmap_filtering = quality_filtering %>% stringr::word(-1, sep = fixed("_"))
  )

# pdf_list_df
pdf_list_df %>% 
  dplyr::select(-file_full_path) %>% 
  dplyr::slice_sample(n=15) %>% 
  dplyr::arrange(study_site,quality_filtering) %>% 
  kableExtra::kbl(caption=paste0("Sample of study site reports extracted from raw data directory (", nrow(pdf_list_df), " files detected)")) %>% 
  kableExtra::kable_styling()
```

## Metashape Report PDF Data Extraction

Define function to extract data from the Agisoft Metashpae `pdf` reports

```{r pdf-read-fn}
### function to extract time value
parse_time_value_fn <- function(val) {
    val = tolower(val)
    # seconds
    seconds = dplyr::case_when(
        stringr::str_detect(val, pattern = "seconds") ~ stringr::word(val, start = 1, sep = "seconds") %>% 
          stringr::str_squish() %>% 
          stringr::word(start = -1)
        , T ~ "0"
      ) %>% 
      as.numeric()
    # minutes
    minutes = dplyr::case_when(
        stringr::str_detect(val, pattern = "minutes") ~ stringr::word(val, start = 1, sep = "minutes") %>% 
          stringr::str_squish() %>% 
          stringr::word(start = -1)
        , T ~ "0"
      ) %>% 
      as.numeric()  
    # hours
    hours = dplyr::case_when(
        stringr::str_detect(val, pattern = "hours") ~ stringr::word(val, start = 1, sep = "hours") %>% 
          stringr::str_squish() %>% 
          stringr::word(start = -1)
        , T ~ "0"
      ) %>% 
      as.numeric()
    # combine
    time_mins = (seconds/60) + minutes + (hours*60)
    return(time_mins)
}
### function to extract memory value
parse_memory_value_fn <- function(val) {
  val = tolower(val)
    # kb
    kb = dplyr::case_when(
        stringr::str_detect(val, pattern = "kb") ~ stringr::word(val, start = 1, sep = "kb") %>% 
          stringr::str_squish() %>% 
          stringr::word(start = -1)
        , T ~ "0"
      ) %>% 
      as.numeric()
    # mb
    mb = dplyr::case_when(
        stringr::str_detect(val, pattern = "mb") ~ stringr::word(val, start = 1, sep = "mb") %>% 
          stringr::str_squish() %>% 
          stringr::word(start = -1)
        , T ~ "0"
      ) %>% 
      as.numeric()  
    # gb
    gb = dplyr::case_when(
        stringr::str_detect(val, pattern = "gb") ~ stringr::word(val, start = 1, sep = "gb") %>% 
          stringr::str_squish() %>% 
          stringr::word(start = -1)
        , T ~ "0"
      ) %>% 
      as.numeric()
    # combine
    mem_mb = (kb/1000) + mb + (gb*1000)
    return(mem_mb)
}


# read each agisoft metashape report pdf and extract metrics
extract_metashape_report_data_fn <- function(file_path) {
  # read the pdf
  pdf_text_ans = pdftools::pdf_text(file_path)
  ##############################
  # pull data out
  ##############################
    ######################################
    ### page 4 table
    ######################################
      table_st_temp = pdf_text_ans[4] %>% 
        stringr::str_locate("X error") %>% 
        .[1,1]
      table_end_temp = (pdf_text_ans[4] %>%
        stringr::str_locate("Table 3") %>% 
        .[1,1])-1
      # matrix
      table_rows_temp = pdf_text_ans[4] %>% 
        substr(
          start = table_st_temp
          , stop = table_end_temp
        ) %>% 
        stringr::str_split(pattern = fixed("\n"), simplify = T) %>% 
        trimws()
      # are units in m or cm?
      use_m_or_cm = dplyr::case_when(
        stringr::str_detect(table_rows_temp[1,1], "\\(m\\)") ~ "\\(m\\)"
        , stringr::str_detect(table_rows_temp[1,1], "\\(cm\\)") ~ "\\(cm\\)"
        , T ~ ""
      )
      # pull names
      names_temp = table_rows_temp[1,1] %>%
        stringr::str_split(pattern = use_m_or_cm, simplify = T) %>% 
        trimws() %>% 
        stringi::stri_remove_empty_na() %>% 
        stringr::str_replace_all("\\s","_") %>% 
        tolower()
      # pull data
      page4_dta_temp = table_rows_temp[1,2:ncol(table_rows_temp)] %>% 
        stringr::str_replace_all("\\s{2,}", ",") %>% 
        stringi::stri_remove_empty_na() %>% 
        textConnection() %>% 
        read.csv(
          sep = ","
          , header = F
          , col.names = names_temp
        ) %>%
        dplyr::as_tibble() %>% 
        dplyr::mutate(
          dplyr::across(
            .cols = tidyselect::everything()
            , .fns = ~ dplyr::case_when(
              use_m_or_cm == "\\(m\\)" ~ .x
              , use_m_or_cm == "\\(cm\\)" ~ .x/100
              , T ~ as.numeric(NA)
            )
          )
        ) %>% 
        dplyr::rename_with(~ paste0(.x,"_m", recycle0 = TRUE))
    
    ######################################
    ### page 6 table
    ######################################
    page6_dta_temp =
      pdf_text_ans[6] %>% 
        stringr::str_remove("Processing Parameters\n\n") %>% 
        stringr::str_split(pattern = fixed("\n"), simplify = T) %>% 
        trimws() %>% 
        stringr::str_replace_all("\\s{2,}", "|") %>% 
        textConnection() %>% 
        read.csv(
          sep = "|"
          , header = F
          , col.names = c("var", "val")
        ) %>%
        dplyr::as_tibble() %>% 
        dplyr::mutate(
          val = val %>% stringr::str_squish() %>% tolower()
          , is_header = is.na(val) | val == ""
          , heading_grp = cumsum(is_header)
        ) %>% 
        dplyr::group_by(heading_grp) %>% 
        dplyr::mutate(
          heading_nm = dplyr::first(var) %>% 
            tolower() %>% 
            stringr::str_remove_all("parameters") %>% 
            stringr::str_squish() %>% 
            stringr::str_replace_all("\\s", "_")
        ) %>% 
        dplyr::ungroup() %>% 
        dplyr::mutate(
          new_var = paste0(
            heading_nm
            , "_"
            , var %>% tolower() %>% stringr::str_replace_all("\\s", "_")
          )
        ) %>% 
        dplyr::filter(is_header==F) %>% 
        dplyr::select(new_var, val) %>% 
        dplyr::distinct() %>% 
        dplyr::mutate(
          val = dplyr::case_when(
            stringr::str_ends(new_var, "_time") ~ parse_time_value_fn(val) %>% as.character()
            , stringr::str_ends(new_var, "_memory_usage") ~ parse_memory_value_fn(val) %>% as.character()
            , stringr::str_ends(new_var, "_file_size") ~ parse_memory_value_fn(val) %>% as.character()
            , T ~ val
          )
          , new_var = dplyr::case_when(
            stringr::str_ends(new_var, "_time") ~ paste0(new_var, "_mins")
            , stringr::str_ends(new_var, "_memory_usage") ~ paste0(new_var, "_mb")
            , stringr::str_ends(new_var, "_file_size") ~ paste0(new_var, "_mb")
            , T ~ new_var
          )
        ) %>% 
        tidyr::pivot_wider(names_from = new_var, values_from = val) %>% 
        dplyr::mutate(
          dplyr::across(
            .cols = c(
              tidyselect::ends_with("_mins")
              , tidyselect::ends_with("_mb")
              , tidyselect::ends_with("_count")
              , tidyselect::ends_with("_cameras")
              , dense_point_cloud_points
            )
            , .fns = ~ readr::parse_number(.x)
          )
        ) %>% 
        dplyr::mutate(
          total_dense_point_cloud_processing_time_mins = (
              dense_cloud_generation_processing_time_mins +
              depth_maps_generation_processing_time_mins
            )
          , total_sparse_point_cloud_processing_time_mins = (
              alignment_matching_time_mins +
              alignment_alignment_time_mins
            )
        )
  ######################################
  ### full pdf data
  ######################################
  pdf_data_temp =
    dplyr::tibble(
      file_full_path = file_path
      # pdf page 1
      , pdf_title = pdf_text_ans[1] %>% 
          stringr::word(1, sep = fixed("\n"))
      # pdf page 2
      , number_of_images = pdf_text_ans[2] %>% 
        stringr::word(-1, sep = fixed("Number of images:")) %>% 
        stringr::word(1, sep = fixed("Camera stations:")) %>% 
        readr::parse_number()
      , flying_altitude_m = pdf_text_ans[2] %>% 
        stringr::word(-1, sep = fixed("Flying altitude:")) %>% 
        stringr::word(1, sep = fixed(" m ")) %>% 
        readr::parse_number()
      , tie_points = pdf_text_ans[2] %>% 
        stringr::word(-1, sep = fixed("Tie points:")) %>% 
        stringr::word(1, sep = fixed("\n")) %>% 
        readr::parse_number()
      , ground_resolution_cm_pix = pdf_text_ans[2] %>% 
        stringr::word(-1, sep = fixed("Ground resolution:")) %>% 
        stringr::word(1, sep = fixed("cm")) %>% 
        readr::parse_number()
      , coverage_area_km2 = pdf_text_ans[2] %>% 
        stringr::word(-1, sep = fixed("Coverage area:")) %>% 
        stringr::word(1, sep = fixed("km")) %>% 
        readr::parse_number()
      , reprojection_error_pix = pdf_text_ans[2] %>% 
        stringr::word(-1, sep = fixed("Reprojection error:")) %>% 
        stringr::word(1, sep = fixed("pix")) %>% 
        readr::parse_number()
    ) %>% 
    dplyr::bind_cols(page4_dta_temp, page6_dta_temp)
  # return
  return(pdf_data_temp)
}
```

Build a data table using the `pdf` data extraction function for each `pdf` report file found in the raw data directory

```{r pdf-read}
# map function over list of files
pdf_data_temp = pdf_list_df$file_full_path %>% 
  purrr::map(extract_metashape_report_data_fn) %>% 
  dplyr::bind_rows()
# combine with original data
if(nrow(pdf_data_temp) != nrow(pdf_list_df)){stop("extract_metashape_report_data_fn failed...check missing data or duplicated data")}else{
  pdf_list_df = pdf_list_df %>% 
    left_join(pdf_data_temp, by = dplyr::join_by("file_full_path")) %>% 
    dplyr::mutate(
      depth_maps_generation_quality = factor(
          depth_maps_generation_quality
          , ordered = TRUE
          , levels = c(
            "lowest"
            , "low"
            , "medium"
            , "high"
            , "ultra high"
          )
        ) %>% forcats::fct_rev()
      , depth_maps_generation_filtering_mode = factor(
          depth_maps_generation_filtering_mode
          , ordered = TRUE
          , levels = c(
            "disabled"
            , "mild"
            , "moderate"
            , "aggressive"
          )
        ) %>% forcats::fct_rev()
    )
}
```

Write out data

```{r dta-write}
## write out data
pdf_list_df %>% 
  dplyr::select(-c(
    file_full_path, metashape_quality
    , metashape_depthmap_filtering, quality_filtering
    , pdf_title
  )) %>% 
  dplyr::relocate(
    c(
      depth_maps_generation_quality, depth_maps_generation_filtering_mode
      , total_sparse_point_cloud_processing_time_mins
      , total_dense_point_cloud_processing_time_mins
      , dense_point_cloud_points
      , dense_cloud_generation_file_size_mb
      , tidyselect::ends_with("_error_m")
    )
    , .after = study_site
  ) %>% 
  dplyr::arrange(
    study_site, depth_maps_generation_quality, depth_maps_generation_filtering_mode
  ) %>% 
  dplyr::mutate(
    dplyr::across(
      .cols = c(depth_maps_generation_quality, depth_maps_generation_filtering_mode)
      , .fns = ~ stringr::str_to_title(.x)
    )
  ) %>% 
  write.csv(
    file = paste0(rootdir, "/metashape_processing_data.csv")
    , row.names = F
  )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

# Metashape Report Data Exploration

## Preliminaries

What does the data look like?

```{r dta-glimpse}
pdf_list_df %>% dplyr::glimpse()
```

Do the processing settings match the file names?

```{r name-match}
pdf_list_df %>% 
  dplyr::mutate(
    quality_match = toupper(depth_maps_generation_quality) %>% 
      stringr::str_remove_all("\\s") == 
      toupper(metashape_quality) %>% 
        stringr::str_remove_all("\\s")
    , filtering_match = toupper(depth_maps_generation_filtering_mode) %>% 
      stringr::str_remove_all("\\s") == 
      toupper(metashape_depthmap_filtering) %>% 
        stringr::str_remove_all("\\s")
  ) %>% 
  dplyr::count(quality_match, filtering_match) %>% 
  kableExtra::kbl(caption="Do the processing settings match the file names?") %>% 
  kableExtra::kable_styling()
```

How many records are there for each depth map generation quality and depth map filtering mode settings?

```{r quality-filtering}
pdf_list_df %>% 
  dplyr::count(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>% 
  ggplot(mapping = aes(
    x = n
    , y = depth_maps_generation_quality
    , fill = depth_maps_generation_filtering_mode)
  ) +
  geom_col(width = 0.7, alpha = 0.8) +
  geom_text(
    mapping = aes(
      group=depth_maps_generation_filtering_mode
      ,label = scales::comma(n, accuracy = 1)
      , fontface = "bold"
      )
    , position = position_stack(vjust = 0.5)
    , color = "black"
  ) +
  scale_fill_viridis_d(option = "plasma") +
  scale_x_continuous(breaks = scales::extended_breaks(n=14)) +
  labs(
    fill = "Filtering Mode"
    , y = "Quality"
    , x = "n"
  ) +
  theme_light() +
  theme(
    legend.position = "top"
    , legend.direction  = "horizontal"
  ) +
  guides(
    fill = guide_legend(reverse = T, override.aes = list(alpha = 0.9))
  )
```

How many records are there for each study site?

```{r study-site}
pdf_list_df %>% 
  dplyr::count(depth_maps_generation_quality, depth_maps_generation_filtering_mode, study_site) %>% 
  ggplot(mapping = aes(
    x = n
    , y = depth_maps_generation_quality
    , fill = depth_maps_generation_filtering_mode)
  ) +
  geom_col(width = 0.7, alpha = 0.8) +
  geom_text(
    mapping = aes(
      group=depth_maps_generation_filtering_mode
      ,label = scales::comma(n, accuracy = 1)
      , fontface = "bold"
      )
    , position = position_stack(vjust = 0.5)
    , color = "black"
  ) +
  facet_wrap(facets = vars(study_site), ncol = 2) +
  scale_fill_viridis_d(option = "plasma") +
  labs(
    fill = "Filtering Mode"
    , y = "Quality"
    , x = "n"
  ) +
  theme_light() +
  theme(
    legend.position = "top"
    , legend.direction  = "horizontal"
    , strip.text = element_text(color = "black", face = "bold")
  ) +
  guides(
    fill = guide_legend(reverse = T, override.aes = list(alpha = 0.9))
  )
```

## Metashape Processing Time Summary

Processing time by depth map generation quality and depth map filtering mode 

```{r time-quality}
pdf_list_df %>% 
  ggplot(
    mapping = aes(
      x = depth_maps_generation_quality
      , y = total_dense_point_cloud_processing_time_mins
      , color = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
  geom_boxplot(alpha = 0.6) +
  scale_color_viridis_d(option = "plasma") +
  scale_fill_viridis_d(option = "plasma") +
  scale_y_log10(
    labels = scales::comma_format(suffix = " mins", accuracy = 1)
    , breaks = scales::breaks_log(n = 8)
  ) +
  labs(
    color = "Filtering Mode"
    , fill = "Filtering Mode"
    , y = "Dense Cloud + Depth Map Generation Time "
    , x = "Quality"
    , title = "Agisoft Metashape processing time by depth map generation quality and filtering mode"
    , caption = "*Note the log scale on the y-axis"
  ) +
  theme_light() +
  theme(
    legend.position = "top"
    , legend.direction  = "horizontal"
  ) +
  guides(
    color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9))
  )
```

Why is there such a great spread and left skew for the high and ultra high quality?

```{r time-area}
pdf_list_df %>% 
  ggplot(
    mapping = aes(
      y = total_dense_point_cloud_processing_time_mins
      , x = depth_maps_generation_quality
      , color = depth_maps_generation_filtering_mode
    )
  ) +
  geom_point(size = 3, alpha = 0.8) +
  facet_grid(
    cols = vars(study_site)
    , labeller = label_wrap_gen(width = 35, multi_line = TRUE)
  ) +
  scale_color_viridis_d(option = "plasma") +
  scale_y_log10(
    labels = scales::comma_format(suffix = " mins", accuracy = 1)
    , breaks = scales::breaks_log(n = 8)
  ) +
  labs(
    color = "Filtering Mode"
    , fill = "Filtering Mode"
    , y = "Dense Cloud + Depth Map Generation Time "
    , x = "Quality"
    , title = "Agisoft Metashape processing time by depth map generation quality and filtering mode"
    , subtitle = "by Study Site"
    , caption = "*Note the log scale on the y-axis"
  ) +
  theme_light() + 
  theme(
    legend.position = "top"
    , legend.direction  = "horizontal"
    , strip.text = element_text(color = "black", face = "bold")
    , axis.text.x = element_text(angle = 90)
    # , strip.text.y.left = element_text(angle = 0)
    # , strip.placement = "outside"
  ) +
  guides(
    color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9))
  )
```

The study sites "Kaibab_High" and "WA85_02" have faster processing times than the other four sites across all quality settings.

## Flight and Sparse Cloud Metrics

How do the UAS flight settings and sparse cloud generation parameters differ across sites?

```{r site-metrics, fig.height = 9}
pdf_list_df %>% 
  dplyr::filter(quality_filtering=="ULTRAHIGH_MILD") %>% 
  dplyr::select(
    study_site
    , number_of_images
    , tie_points
    , ground_resolution_cm_pix
    , flying_altitude_m
    , coverage_area_km2
    , tidyselect::contains("_error_m")
  ) %>% 
  tidyr::pivot_longer(
    cols = -c(study_site), names_to = "metric", values_to = "val"
  ) %>% 
  ggplot(
    mapping = aes(
      x = val
      , y = study_site
      , fill = metric
    )
  ) +
  geom_col(width = 0.7, alpha = 0.8) +
  facet_wrap(facets = vars(metric), ncol = 2, scales = "free_x") +
  scale_fill_viridis_d(option = "cividis") +
  labs(
    y = ""
    , x = ""
    , title = "Different processing metrics by study site"
    , subtitle = "`Ultra High` quality and `Mild` filtering used where applicable (error, resolution, coverage)"
  ) +
  theme_light() + 
  theme(
    legend.position = "none"
    , strip.text = element_text(color = "black", face = "bold")
  )
```

The study sites "Kaibab_High" and "WA85_02" have smaller coverage areas, fewer images, and higher x error values than the other four sites.

## Metashape Dense Point Cloud Summary

Dense point cloud number of points by depth map generation quality and depth map filtering mode 

```{r pts-quality}
pdf_list_df %>% 
  ggplot(
    mapping = aes(
      x = depth_maps_generation_quality
      , y = dense_point_cloud_points
      , color = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
  geom_boxplot(alpha = 0.6) +
  scale_color_viridis_d(option = "plasma") +
  scale_fill_viridis_d(option = "plasma") +
  scale_y_log10(
    labels = scales::comma_format(suffix = " M", scale = 1e-6, accuracy = 1)
    , breaks = scales::breaks_log(n = 6)
  ) +
  labs(
    color = "Filtering Mode"
    , fill = "Filtering Mode"
    , y = "Dense Point Cloud # Points"
    , x = "Quality"
    , title = "Dense point cloud number of points by depth map generation quality and filtering mode"
    , caption = "*Note the log scale on the y-axis"
  ) +
  theme_light() +
  theme(
    legend.position = "top"
    , legend.direction  = "horizontal"
  ) +
  guides(
    color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9))
  )
```

Notice there are some outlier study sites in the number of dense cloud points

```{r pts-area}
pdf_list_df %>% 
  ggplot(
    mapping = aes(
      y = dense_point_cloud_points
      , x = depth_maps_generation_quality
      , color = depth_maps_generation_filtering_mode
    )
  ) +
  geom_point(size = 3, alpha = 0.8) +
  facet_grid(
    cols = vars(study_site)
    , labeller = label_wrap_gen(width = 35, multi_line = TRUE)
  ) +
  scale_color_viridis_d(option = "plasma") +
  scale_y_log10(
    labels = scales::comma_format(suffix = " M", scale = 1e-6, accuracy = 1)
    , breaks = scales::breaks_log(n = 6)
  ) +
  labs(
    color = "Filtering Mode"
    , y = "Dense Point Cloud # Points"
    , x = "Quality"
    , title = "Dense point cloud number of points by depth map generation quality and filtering mode"
    , subtitle = "by Study Site"
    , caption = "*Note the log scale on the y-axis"
  ) +
  theme_light() + 
  theme(
    legend.position = "top"
    , legend.direction  = "horizontal"
    , strip.text = element_text(color = "black", face = "bold")
    , axis.text.x = element_text(angle = 90)
  ) +
  guides(
    color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9))
  )
```

The study site "Kaibab_Low" has fewer dense cloud points than the other five sites for all filtering modes in the "ultra high" and "high" quality settings. The study site "WA85_02" has more dense cloud points than the other five sites for all filtering modes in the "ultra high" quality setting but similar point numbers for the other processing settings.

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

# **R** Point Cloud Processing{#ptcld_analysis}

After running the UAS point cloud [processing script](https://github.com/georgewoolsey/metashape_testing/blob/2962a3cd2935a5271a207aff9ce92323b4e258b8/src/software_point_cloud_processing.R) in **R**...the processing tracking data file is used to compare point cloud processing times.

```{r processing-dta-ld, results='hide'}
ptcld_processing_data = readr::read_csv(file = "../data/ptcld_processing_tracking_data.csv") %>% 
  dplyr::rename(
    depth_maps_generation_quality = processing_attribute1
    , depth_maps_generation_filtering_mode = processing_attribute2
  ) %>% 
  dplyr::mutate(
    depth_maps_generation_quality = factor(
        depth_maps_generation_quality %>% 
          tolower() %>% 
          stringr::str_replace_all("ultrahigh", "ultra high")
        , ordered = TRUE
        , levels = c(
          "lowest"
          , "low"
          , "medium"
          , "high"
          , "ultra high"
        )
      ) %>% forcats::fct_rev()
    , depth_maps_generation_filtering_mode = factor(
        depth_maps_generation_filtering_mode %>% tolower()
        , ordered = TRUE
        , levels = c(
          "disabled"
          , "mild"
          , "moderate"
          , "aggressive"
        )
      ) %>% forcats::fct_rev()
  )
```

## Processing Time Summary

Total processing time by depth map generation quality and depth map filtering mode 

```{r processing-dta-tot}
ptcld_processing_data %>% 
  ggplot(
    mapping = aes(
      x = depth_maps_generation_quality
      , y = timer_total_time_mins
      , color = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
  geom_boxplot(alpha = 0.6) +
  scale_color_viridis_d(option = "plasma") +
  scale_fill_viridis_d(option = "plasma") +
  scale_y_log10(
    labels = scales::comma_format(suffix = " mins", accuracy = 1)
    , breaks = scales::breaks_log(n = 9)
  ) +
  labs(
    color = "Filtering Mode"
    , fill = "Filtering Mode"
    , y = "Point Cloud Total Processing Time"
    , x = "Quality"
    , title = bquote(
        bold("R") ~
        "point cloud total processing time by depth map generation quality and filtering mode"
    )
    , caption = "*Note the log scale on the y-axis"
  ) +
  theme_light() +
  theme(
    legend.position = "top"
    , legend.direction  = "horizontal"
  ) +
  guides(
    color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9))
  )
```

Notice there are some outlier study sites in the point cloud processing time

```{r processing-dta-tot-area}
ptcld_processing_data %>% 
  ggplot(
    mapping = aes(
      y = timer_total_time_mins
      , x = depth_maps_generation_quality
      , color = depth_maps_generation_filtering_mode
    )
  ) +
  geom_point(size = 3, alpha = 0.8) +
  facet_grid(
    cols = vars(study_site)
    , labeller = label_wrap_gen(width = 35, multi_line = TRUE)
  ) +
  scale_color_viridis_d(option = "plasma") +
  scale_y_log10(
    labels = scales::comma_format(suffix = " mins", accuracy = 1)
    , breaks = scales::breaks_log(n = 9)
  ) +
  labs(
    color = "Filtering Mode"
    , y = "Point Cloud Total Processing Time"
    , x = "Quality"
    , title = bquote(
        bold("R") ~
        "point cloud total processing time by depth map generation quality and filtering mode"
    )
    , subtitle = "by Study Site"
    , caption = "*Note the log scale on the y-axis"
  ) +
  theme_light() + 
  theme(
    legend.position = "top"
    , legend.direction  = "horizontal"
    , strip.text = element_text(color = "black", face = "bold")
    , axis.text.x = element_text(angle = 90)
  ) +
  guides(
    color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9))
  )
```

## Processing Time vs # Points

```{r processing-dta-time-pts}
ptcld_processing_data %>% 
  ggplot(
    mapping = aes(
      x = number_of_points
      , y = timer_total_time_mins
    )
  ) +
  geom_point(alpha = 0.7, color = "navy") +
  scale_y_log10(
    labels = scales::comma_format(suffix = " mins", accuracy = 1)
    , breaks = scales::breaks_log(n = 9)
  ) +
  scale_x_log10(
    labels = scales::comma_format(suffix = " M", scale = 1e-6, accuracy = 1)
    , breaks = scales::breaks_log(n = 6)
  ) +
  labs(
    y = "Point Cloud Total Processing Time"
    , x = "Dense Point Cloud # Points"
    , title = bquote(
        bold("R") ~
        "point cloud total processing time versus dense point cloud number of points"
    )
    , caption = "*Note the log scale on both axes"
  ) +
  theme_light()
```

## Processing Section Timing

```{r time-breakdown}
ptcld_processing_data %>% 
  dplyr::select(
    depth_maps_generation_quality
    , tidyselect::ends_with("_mins")
  ) %>% 
  dplyr::select(-c(timer_total_time_mins)) %>% 
  tidyr::pivot_longer(
    cols = -c(depth_maps_generation_quality)
    , names_to = "section"
    , values_to = "mins"
  ) %>% 
  # dplyr::count(depth_maps_generation_quality, section)
  dplyr::group_by(depth_maps_generation_quality, section) %>% 
  dplyr::summarise(med_mins = median(mins)) %>% 
  dplyr::group_by(depth_maps_generation_quality) %>% 
  dplyr::mutate(
    total_mins = sum(med_mins)
    , pct_mins = med_mins/total_mins 
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(
    section = section %>% 
      stringr::str_remove_all("timer_") %>% 
      stringr::str_remove_all("_time_mins") %>% 
      factor(
        ordered = T
        , levels = c(
          "tile"
          , "denoise"
          , "classify"
          , "dtm"
          , "normalize"
          , "chm"
          , "treels"
          , "itd"
          , "estdbh"
          , "competition"
          , "silv"
        )
        , labels = c(
          "Tile"
          , "Denoise"
          , "Classify"
          , "DTM"
          , "Normalize"
          , "CHM"
          , "TreeLS SfM DBH"
          , "CHM I.T.D."
          , "Local DBH Est."
          , "Tree Competition"
          , "Silvicultural Metrics"
        )
      ) %>% forcats::fct_rev()
  ) %>%
  
ggplot(
  mapping = aes(x = pct_mins, y = depth_maps_generation_quality, fill=section, group=section)
) +
  geom_col(
    width = 0.7, alpha=0.8
  ) +
  geom_text(
    mapping = aes(
        label = scales::percent(ifelse(pct_mins>=0.06,pct_mins,NA), accuracy = 1)
        , fontface = "bold"
      )
    , position = position_stack(vjust = 0.5)
    , color = "black", size = 4
  ) +
  scale_fill_viridis_d(option = "turbo") +
  scale_x_continuous(labels = scales::percent_format()) +
  labs(
    fill = "R script\nsection"
    , y = "Metashape depth map quality"
    , x = "% Point Cloud Total Processing Time"
    , title = bquote(
        bold("R") ~
        "point cloud total processing time by Metashape depth map generation quality and R script section"
    )
    , subtitle = "Median across study site and Metashape depth map filtering mode "
  ) +
  theme_light() +
  theme(
    legend.position = "top"
    , legend.direction  = "horizontal"
    , legend.title = element_text(size=7)
    , axis.title.x = element_text(size=10, face = "bold")
    , axis.title.y = element_text(size = 8)
    , axis.text.x = element_blank()
    , axis.text.y = element_text(color = "black",size=10, face = "bold")
    , axis.ticks.x = element_blank()
  ) +
  guides(
    fill = guide_legend(nrow = 3, byrow = T, reverse = T, override.aes = list(alpha = 0.9))
  )  
```

# Statistical Analysis

The objective of this study is to determine the influence of different structure from motion (SfM) software (e.g. Agisoft  Metashap, OpenDroneMap, Pix4D) and processing parameters on processing and forest measurement outcomes. All of the predictor variables of interest in this study are categorical (i.e. factor or nominal) while the predicted variables are metric and include processing time (continuous > 0) and F-score (ranges from 0-1). This type of statistical analysis is described in the second edition of Kruschke's [*Doing Bayesian data analysis* (2015)](https://sites.google.com/site/doingbayesiandataanalysis/):

> This chapter considers data structures that consist of a metric predicted variable and two (or more) nominal predictors....Data structures of the type considered in this chapter are often encountered in real research. For example, we might want to predict monetary income from political party affiliation and religious affiliation, or we might want to predict galvanic skin response to different combinations of categories of visual stimulus and categories of auditory stimulus. As mentioned in the previous chapter, this type of data structure can arise from experiments or from observational studies. In experiments, the researcher assigns the categories (at random) to the experimental subjects. In observational studies, both the nominal predictor values and the metric predicted value are generated by processes outside the direct control of the researcher.
>
>The traditional treatment of this sort of data structure is called multifactor analysis of variance (ANOVA). Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups. [Kruschke (2015, pp.583--584)](https://sites.google.com/site/doingbayesiandataanalysis/)

The following analysis will expand the traditional mixed ANOVA approach following the methods outlined by Kassambara in the [*Comparing Multiple Means in R*](https://www.datanovia.com/en/courses/comparing-multiple-means-in-r/) online course to build a Bayesian approach based on [Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/). This analysis was greatly enhanced by [A. Solomon Kurz's ebook supplement](https://solomonkurz.netlify.app/book/) to [Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/).

## Predicting Point Cloud Processing Time

This data was already described in [this section above](#ptcld_analysis).

### One Nominal Predictor

We'll start by exploring the influence of the depth maps depth map qualityneration quality parameter on th, y = "gepoint cloud processing mins."e point cloud processing time.

Summary statistics by group:

```{r one-sum-stats}
ptcld_processing_data %>% 
  dplyr::group_by(depth_maps_generation_quality) %>% 
  dplyr::summarise(
    mean_processing_mins = mean(timer_total_time_mins, na.rm = T)
    # , med_processing_mins = median(timer_total_time_mins, na.rm = T)
    , sd_processing_mins = sd(timer_total_time_mins, na.rm = T)
    , n = dplyr::n()
  ) %>% 
  kableExtra::kbl(digits = 1, caption = "summary statistics: point cloud processing time by depth map quality") %>% 
  kableExtra::kable_styling()
```

We can use a linear model to obtain means by group:

```{r one-sum-lm}
lm1_temp = lm(
  timer_total_time_mins ~ 0 + depth_maps_generation_quality
  , data = ptcld_processing_data
)

# summary
lm1_temp %>% 
  broom::tidy() %>% 
  mutate(term = stringr::str_remove_all(term, "depth_maps_generation_quality")) %>% 
  kableExtra::kbl(digits = 2, caption = "linear model: point cloud processing time by depth map quality") %>% 
  kableExtra::kable_styling()
```

and plot these means with 95% confidence interval

```{r}
lm1_temp %>%
  broom::tidy() %>% 
  dplyr::bind_cols(
    lm1_temp %>% 
      confint() %>% 
      dplyr::as_tibble() %>% 
      dplyr::rename(lower = 1, upper = 2)
  ) %>%
  mutate(
    term = term %>% 
      stringr::str_remove_all("depth_maps_generation_quality") %>% 
      factor(
          ordered = TRUE
          , levels = c(
            "lowest"
            , "low"
            , "medium"
            , "high"
            , "ultra high"
          )
        ) %>% forcats::fct_rev()
  ) %>% 
  ggplot(
    mapping = aes(x = term, y = estimate, fill = term)
  ) +
  geom_col() + 
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, color = "gray66") +
  scale_fill_viridis_d(option = "inferno") +
  scale_y_continuous(breaks = scales::extended_breaks(n=8)) +
  labs(x = "depth map quality", y = "point cloud processing time") +
  theme_light() +
  theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

Finally, one-way ANOVA 

```{r one-sum-aov}
aov1_temp = aov(
  timer_total_time_mins ~ 0 + depth_maps_generation_quality
  , data = ptcld_processing_data
) 
# summary
aov1_temp %>% 
  broom::tidy() %>% 
  kableExtra::kbl(digits = 2, caption = "one-way ANOVA: point cloud processing time by depth map quality") %>% 
  kableExtra::kable_styling()
```

The sum of squared residuals is the same between the linear model and the ANOVA model

```{r one-sum-chk}
# RSS
identical(
  # linear model
  lm1_temp$residuals %>% 
    dplyr::as_tibble() %>% 
    mutate(value=value^2) %>% 
    dplyr::pull(value) %>% 
    sum()
  # anova
  , summary(aov1_temp)[[1]][["Sum Sq"]][[2]]
)
# F value
identical(
  # linear model
  summary(lm1_temp)$fstatistic["value"] %>% unname() %>% round(6)
  # anova
  , summary(aov1_temp)[[1]][["F value"]][[1]] %>% unname() %>% round(6)
)
```

[Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/) notes: 

> The terminology, "analysis of variance," comes from a decomposition of overall data variance into within-group variance and between-group variance (Fisher, 1925). Algebraically, the sum of squared deviations of the scores from their overall mean equals the sum of squared deviations of the scores from their respective group means plus the sum of squared deviations of the group means from the overall mean. In other words, the total variance can be partitioned into within-group variance plus between-group variance. Because one definition of the word "analysis" is separation into constituent parts, the term ANOVA accurately describes the underlying algebra in the traditional methods. That algebraic relation is not used in the hierarchical Bayesian approach presented here. The Bayesian method can estimate component variances, however. Therefore, the Bayesian approach is not ANOVA, but is analogous to ANOVA. (p. 556)

and see section 19 from [Kurz's ebook supplement](https://bookdown.org/content/3686/metric-predicted-variable-with-one-nominal-predictor.html#hierarchical-bayesian-approach)

The metric predicted variable with one nominal predictor variable model has the form:

\begin{align*}
y_{i}  &\sim {\sf Normal} \bigl(\mu_{i}, \sigma_{y} \bigr) \\
\mu_{i} &= \beta_0 + \sum_{j=1}^{J} \beta_{1[j]} x_{1[j]} \bigl(i\bigr) \\
\beta_{0}  &\sim {\sf Normal} (0,10) \\ 
\beta_{1[j]}  &\sim {\sf Normal} (0,\sigma_{\beta_{1}}) \\ 
\sigma_{\beta_{1}} &\sim {\sf uniform} (0,100) \\ 
\end{align*}

, where $j$ is the depth map generation quality setting corresponding to observation $i$ 

```{r one-sum-brms}
brms1_mod = brms::brm(
  formula = timer_total_time_mins ~ 0 + depth_maps_generation_quality
  , data = ptcld_processing_data
  , family = brms::brmsfamily(family = "gaussian")
  , iter = 4000, warmup = 1000, chains = 4
  , file = paste0(rootdir, "/fits/brms1_mod")
)
```

The `brms::brm` model summary

```{r one-sum-brms-rslt1}
brms1_mod %>% 
  brms::posterior_summary() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>% 
  dplyr::filter(stringr::str_starts(parameter, "b_") | parameter == "sigma") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::mutate(parameter = stringr::str_remove_all(parameter,"b_depth_maps_generation_quality")) %>% 
  kableExtra::kbl(digits = 2, caption = "Bayesian one nominal predictor: point cloud processing time by depth map quality") %>% 
  kableExtra::kable_styling()
```

```{r}
# extract the posterior draws
brms::as_draws_df(brms1_mod) %>% 
# plot
  ggplot(aes(x = sigma, y = 0)) +
  tidybayes::stat_dotsinterval(
    point_interval = mode_hdi
    , .width = .95
    , justification = -0.04
    , shape = 21
    , stroke = 1/4, point_size = 3, slab_size = 1/4
    # , quantiles = 100
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(latex2exp::TeX("$\\sigma_y$")) +
  theme_light()
```


```{r one-sum-brms-rslt2, include=FALSE, eval=F}
brms1_mod %>% 
  plot(variable = c("^b_", "sigma"), regex = TRUE)

brms::as_draws_df(
        brms1_mod
        , variable = c("^b_", "shape")
        , regex = TRUE
      ) %>% 
      # quick way to get a table of summary statistics and diagnostics
      posterior::summarize_draws(
        "mean", "median", "sd"
        ,  ~quantile(.x, probs = c(
          0.05, 0.95
          # , 0.025, 0.975
        ))
        , "rhat"
      )
```



```{r one-sum-norm, eval=FALSE, include=FALSE}
# Check normality assumption by groups. Computing Shapiro-Wilk test for each group level. If the data is normally distributed, the p-value should be greater than 0.05.


ptcld_processing_data %>% 
  dplyr::group_by(depth_maps_generation_quality) %>% 
  rstatix::shapiro_test(timer_total_time_mins) %>% 
  kableExtra::kbl(digits = 3) %>% 
  kableExtra::kable_styling()

# The time is not normally distributed (p < 0.05) for most groups as assessed by Shapiro-Wilk’s test of normality. 
# 
# If you have doubt about the normality of the data, you can use the Kruskal-Wallis test, which is the non-parametric alternative to one-way ANOVA test.
```

