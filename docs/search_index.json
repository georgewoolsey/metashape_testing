[["index.html", "SfM Processing Software Comparison Section 1 Introduction", " SfM Processing Software Comparison George Woolsey 29 February, 2024 Section 1 Introduction The objective of this study is to determine the influence of different Agisoft Metashape structure from motion (SfM) software (e.g. Agisoft Metashap, OpenDroneMap, Pix4D) and processing parameters on processing and forest measurement outcomes. This analysis builds on Tinkham and Swayze (2021) by including UAS flights from different forests and by also comparing different SfM processing software. UAS flights from the follwing forests were included: the Manitou Experimental Forest on the Pike-San Isabel National Forest (Colorado; “N1”), the Black Hills Experimental Forest on the Black Hills National Forest (South Dakota; “SQ02_04”, “SQ09_02”, “WA85_02”), and the Lookout Canyon area in the Kaibab National Forest (Arizona; “Kaibab_High”, “Kaibab_Low”). "],["sfm_data.html", "Section 2 SfM Image Processing Data 2.1 User-Defined Parameters 2.2 Metashape Image Processing 2.3 Metashape Report Data Exploration", " Section 2 SfM Image Processing Data This section extracts data from the SfM image processing software reports (usually in pdf format) and reports summary statistics on processing time. 2.1 User-Defined Parameters Parameters to be set by the user # !!!!!!!!!!!!!!!!!!!!!!! USER-DEFINED PARAMETERS !!!!!!!!!!!!!!!!!!!!!!! # ###____________________### ### Set directory for outputs ### ###____________________### # rootdir = &quot;../data&quot; rootdir = &quot;../data&quot; ###_________________________### ### Set input data directory ### ###_________________________### # !!!!!!!!!! this is where both pdf report data resides # !!!!!!!!!! files should be named in the format {quality}_{depth map filtering}.pdf # !!!!!!!!!! for example: high_aggressive.pdf; UltraHigh_Disabled.pdf; lowest_MILD.pdf input_data_dir = &quot;../data/raw_data&quot; ###_________________________### ### list of study site directory names ###_________________________### # !!!!!!!!!! this is where both pdf and las data reside # directories matching the names of these study sites will be searched study_site_list = c( &quot;SQ02_04&quot;, &quot;SQ09_02&quot;, &quot;WA85_02&quot; , &quot;Kaibab_High&quot;, &quot;Kaibab_Low&quot; , &quot;n1&quot; ) # !!!!!!!!!!!!!!!!!!!!!!! USER-DEFINED PARAMETERS !!!!!!!!!!!!!!!!!!!!!!! # 2.2 Metashape Image Processing Search for the list of Agisoft Metashape processing report pdf files to extract information from in the user-defined input_data_dir directory. Parse the list of files to extract processing information and study site information from. # get list of files and directories to read from pdf_list = list.files(normalizePath(input_data_dir), pattern = &quot;.*\\\\.(pdf)$&quot;, full.names = T, recursive = T) # set up data.frame for processing pdf_list_df = dplyr::tibble( file_full_path = pdf_list ) %&gt;% dplyr::mutate( study_site = file_full_path %&gt;% stringr::word(-1, sep = fixed(normalizePath(input_data_dir))) %&gt;% toupper() %&gt;% stringr::str_extract(pattern = paste(toupper(study_site_list),collapse = &quot;|&quot;)) , quality_filtering = file_full_path %&gt;% stringr::word(-1, sep = fixed(&quot;/&quot;)) %&gt;% stringr::word(1, sep = fixed(&quot;.&quot;)) %&gt;% toupper() , metashape_quality = quality_filtering %&gt;% stringr::word(1, sep = fixed(&quot;_&quot;)) , metashape_depthmap_filtering = quality_filtering %&gt;% stringr::word(-1, sep = fixed(&quot;_&quot;)) ) # pdf_list_df pdf_list_df %&gt;% dplyr::select(-file_full_path) %&gt;% dplyr::slice_sample(n=15) %&gt;% dplyr::arrange(study_site,quality_filtering) %&gt;% kableExtra::kbl(caption=paste0(&quot;Sample of study site reports extracted from raw data directory (&quot;, nrow(pdf_list_df), &quot; files detected)&quot;)) %&gt;% kableExtra::kable_styling() Table 2.1: Sample of study site reports extracted from raw data directory (120 files detected) study_site quality_filtering metashape_quality metashape_depthmap_filtering KAIBAB_HIGH HIGH_DISABLED HIGH DISABLED KAIBAB_HIGH HIGH_MILD HIGH MILD KAIBAB_HIGH HIGH_MODERATE HIGH MODERATE KAIBAB_HIGH LOW_AGGRESSIVE LOW AGGRESSIVE KAIBAB_HIGH LOW_DISABLED LOW DISABLED KAIBAB_LOW MEDIUM_DISABLED MEDIUM DISABLED N1 LOW_MILD LOW MILD SQ02_04 LOWEST_AGGRESSIVE LOWEST AGGRESSIVE SQ02_04 LOW_MILD LOW MILD SQ02_04 MEDIUM_DISABLED MEDIUM DISABLED SQ09_02 HIGH_DISABLED HIGH DISABLED SQ09_02 HIGH_MILD HIGH MILD SQ09_02 LOWEST_MODERATE LOWEST MODERATE SQ09_02 MEDIUM_MODERATE MEDIUM MODERATE WA85_02 LOWEST_DISABLED LOWEST DISABLED 2.2.1 Metashape Report PDF Data Extraction Define function to extract data from the Agisoft Metashpae pdf reports ### function to extract time value parse_time_value_fn &lt;- function(val) { val = tolower(val) # seconds seconds = dplyr::case_when( stringr::str_detect(val, pattern = &quot;seconds&quot;) ~ stringr::word(val, start = 1, sep = &quot;seconds&quot;) %&gt;% stringr::str_squish() %&gt;% stringr::word(start = -1) , T ~ &quot;0&quot; ) %&gt;% as.numeric() # minutes minutes = dplyr::case_when( stringr::str_detect(val, pattern = &quot;minutes&quot;) ~ stringr::word(val, start = 1, sep = &quot;minutes&quot;) %&gt;% stringr::str_squish() %&gt;% stringr::word(start = -1) , T ~ &quot;0&quot; ) %&gt;% as.numeric() # hours hours = dplyr::case_when( stringr::str_detect(val, pattern = &quot;hours&quot;) ~ stringr::word(val, start = 1, sep = &quot;hours&quot;) %&gt;% stringr::str_squish() %&gt;% stringr::word(start = -1) , T ~ &quot;0&quot; ) %&gt;% as.numeric() # combine time_mins = (seconds/60) + minutes + (hours*60) return(time_mins) } ### function to extract memory value parse_memory_value_fn &lt;- function(val) { val = tolower(val) # kb kb = dplyr::case_when( stringr::str_detect(val, pattern = &quot;kb&quot;) ~ stringr::word(val, start = 1, sep = &quot;kb&quot;) %&gt;% stringr::str_squish() %&gt;% stringr::word(start = -1) , T ~ &quot;0&quot; ) %&gt;% as.numeric() # mb mb = dplyr::case_when( stringr::str_detect(val, pattern = &quot;mb&quot;) ~ stringr::word(val, start = 1, sep = &quot;mb&quot;) %&gt;% stringr::str_squish() %&gt;% stringr::word(start = -1) , T ~ &quot;0&quot; ) %&gt;% as.numeric() # gb gb = dplyr::case_when( stringr::str_detect(val, pattern = &quot;gb&quot;) ~ stringr::word(val, start = 1, sep = &quot;gb&quot;) %&gt;% stringr::str_squish() %&gt;% stringr::word(start = -1) , T ~ &quot;0&quot; ) %&gt;% as.numeric() # combine mem_mb = (kb/1000) + mb + (gb*1000) return(mem_mb) } # read each agisoft metashape report pdf and extract metrics extract_metashape_report_data_fn &lt;- function(file_path) { # read the pdf pdf_text_ans = pdftools::pdf_text(file_path) ############################## # pull data out ############################## ###################################### ### page 4 table ###################################### table_st_temp = pdf_text_ans[4] %&gt;% stringr::str_locate(&quot;X error&quot;) %&gt;% .[1,1] table_end_temp = (pdf_text_ans[4] %&gt;% stringr::str_locate(&quot;Table 3&quot;) %&gt;% .[1,1])-1 # matrix table_rows_temp = pdf_text_ans[4] %&gt;% substr( start = table_st_temp , stop = table_end_temp ) %&gt;% stringr::str_split(pattern = fixed(&quot;\\n&quot;), simplify = T) %&gt;% trimws() # are units in m or cm? use_m_or_cm = dplyr::case_when( stringr::str_detect(table_rows_temp[1,1], &quot;\\\\(m\\\\)&quot;) ~ &quot;\\\\(m\\\\)&quot; , stringr::str_detect(table_rows_temp[1,1], &quot;\\\\(cm\\\\)&quot;) ~ &quot;\\\\(cm\\\\)&quot; , T ~ &quot;&quot; ) # pull names names_temp = table_rows_temp[1,1] %&gt;% stringr::str_split(pattern = use_m_or_cm, simplify = T) %&gt;% trimws() %&gt;% stringi::stri_remove_empty_na() %&gt;% stringr::str_replace_all(&quot;\\\\s&quot;,&quot;_&quot;) %&gt;% tolower() # pull data page4_dta_temp = table_rows_temp[1,2:ncol(table_rows_temp)] %&gt;% stringr::str_replace_all(&quot;\\\\s{2,}&quot;, &quot;,&quot;) %&gt;% stringi::stri_remove_empty_na() %&gt;% textConnection() %&gt;% read.csv( sep = &quot;,&quot; , header = F , col.names = names_temp ) %&gt;% dplyr::as_tibble() %&gt;% dplyr::mutate( dplyr::across( .cols = tidyselect::everything() , .fns = ~ dplyr::case_when( use_m_or_cm == &quot;\\\\(m\\\\)&quot; ~ .x , use_m_or_cm == &quot;\\\\(cm\\\\)&quot; ~ .x/100 , T ~ as.numeric(NA) ) ) ) %&gt;% dplyr::rename_with(~ paste0(.x,&quot;_m&quot;, recycle0 = TRUE)) ###################################### ### page 6 table ###################################### page6_dta_temp = pdf_text_ans[6] %&gt;% stringr::str_remove(&quot;Processing Parameters\\n\\n&quot;) %&gt;% stringr::str_split(pattern = fixed(&quot;\\n&quot;), simplify = T) %&gt;% trimws() %&gt;% stringr::str_replace_all(&quot;\\\\s{2,}&quot;, &quot;|&quot;) %&gt;% textConnection() %&gt;% read.csv( sep = &quot;|&quot; , header = F , col.names = c(&quot;var&quot;, &quot;val&quot;) ) %&gt;% dplyr::as_tibble() %&gt;% dplyr::mutate( val = val %&gt;% stringr::str_squish() %&gt;% tolower() , is_header = is.na(val) | val == &quot;&quot; , heading_grp = cumsum(is_header) ) %&gt;% dplyr::group_by(heading_grp) %&gt;% dplyr::mutate( heading_nm = dplyr::first(var) %&gt;% tolower() %&gt;% stringr::str_remove_all(&quot;parameters&quot;) %&gt;% stringr::str_squish() %&gt;% stringr::str_replace_all(&quot;\\\\s&quot;, &quot;_&quot;) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( new_var = paste0( heading_nm , &quot;_&quot; , var %&gt;% tolower() %&gt;% stringr::str_replace_all(&quot;\\\\s&quot;, &quot;_&quot;) ) ) %&gt;% dplyr::filter(is_header==F) %&gt;% dplyr::select(new_var, val) %&gt;% dplyr::distinct() %&gt;% dplyr::mutate( val = dplyr::case_when( stringr::str_ends(new_var, &quot;_time&quot;) ~ parse_time_value_fn(val) %&gt;% as.character() , stringr::str_ends(new_var, &quot;_memory_usage&quot;) ~ parse_memory_value_fn(val) %&gt;% as.character() , stringr::str_ends(new_var, &quot;_file_size&quot;) ~ parse_memory_value_fn(val) %&gt;% as.character() , T ~ val ) , new_var = dplyr::case_when( stringr::str_ends(new_var, &quot;_time&quot;) ~ paste0(new_var, &quot;_mins&quot;) , stringr::str_ends(new_var, &quot;_memory_usage&quot;) ~ paste0(new_var, &quot;_mb&quot;) , stringr::str_ends(new_var, &quot;_file_size&quot;) ~ paste0(new_var, &quot;_mb&quot;) , T ~ new_var ) ) %&gt;% tidyr::pivot_wider(names_from = new_var, values_from = val) %&gt;% dplyr::mutate( dplyr::across( .cols = c( tidyselect::ends_with(&quot;_mins&quot;) , tidyselect::ends_with(&quot;_mb&quot;) , tidyselect::ends_with(&quot;_count&quot;) , tidyselect::ends_with(&quot;_cameras&quot;) , dense_point_cloud_points ) , .fns = ~ readr::parse_number(.x) ) ) %&gt;% dplyr::mutate( total_dense_point_cloud_processing_time_mins = ( dense_cloud_generation_processing_time_mins + depth_maps_generation_processing_time_mins ) , total_sparse_point_cloud_processing_time_mins = ( alignment_matching_time_mins + alignment_alignment_time_mins ) ) ###################################### ### full pdf data ###################################### pdf_data_temp = dplyr::tibble( file_full_path = file_path # pdf page 1 , pdf_title = pdf_text_ans[1] %&gt;% stringr::word(1, sep = fixed(&quot;\\n&quot;)) # pdf page 2 , number_of_images = pdf_text_ans[2] %&gt;% stringr::word(-1, sep = fixed(&quot;Number of images:&quot;)) %&gt;% stringr::word(1, sep = fixed(&quot;Camera stations:&quot;)) %&gt;% readr::parse_number() , flying_altitude_m = pdf_text_ans[2] %&gt;% stringr::word(-1, sep = fixed(&quot;Flying altitude:&quot;)) %&gt;% stringr::word(1, sep = fixed(&quot; m &quot;)) %&gt;% readr::parse_number() , tie_points = pdf_text_ans[2] %&gt;% stringr::word(-1, sep = fixed(&quot;Tie points:&quot;)) %&gt;% stringr::word(1, sep = fixed(&quot;\\n&quot;)) %&gt;% readr::parse_number() , ground_resolution_cm_pix = pdf_text_ans[2] %&gt;% stringr::word(-1, sep = fixed(&quot;Ground resolution:&quot;)) %&gt;% stringr::word(1, sep = fixed(&quot;cm&quot;)) %&gt;% readr::parse_number() , coverage_area_km2 = pdf_text_ans[2] %&gt;% stringr::word(-1, sep = fixed(&quot;Coverage area:&quot;)) %&gt;% stringr::word(1, sep = fixed(&quot;km&quot;)) %&gt;% readr::parse_number() , reprojection_error_pix = pdf_text_ans[2] %&gt;% stringr::word(-1, sep = fixed(&quot;Reprojection error:&quot;)) %&gt;% stringr::word(1, sep = fixed(&quot;pix&quot;)) %&gt;% readr::parse_number() ) %&gt;% dplyr::bind_cols(page4_dta_temp, page6_dta_temp) # return return(pdf_data_temp) } Build a data table using the pdf data extraction function for each pdf report file found in the raw data directory # map function over list of files pdf_data_temp = pdf_list_df$file_full_path %&gt;% purrr::map(extract_metashape_report_data_fn) %&gt;% dplyr::bind_rows() # combine with original data if(nrow(pdf_data_temp) != nrow(pdf_list_df)){stop(&quot;extract_metashape_report_data_fn failed...check missing data or duplicated data&quot;)}else{ pdf_list_df = pdf_list_df %&gt;% left_join(pdf_data_temp, by = dplyr::join_by(&quot;file_full_path&quot;)) %&gt;% dplyr::mutate( depth_maps_generation_quality = factor( depth_maps_generation_quality , ordered = TRUE , levels = c( &quot;lowest&quot; , &quot;low&quot; , &quot;medium&quot; , &quot;high&quot; , &quot;ultra high&quot; ) ) %&gt;% forcats::fct_rev() , depth_maps_generation_filtering_mode = factor( depth_maps_generation_filtering_mode , ordered = TRUE , levels = c( &quot;disabled&quot; , &quot;mild&quot; , &quot;moderate&quot; , &quot;aggressive&quot; ) ) %&gt;% forcats::fct_rev() ) } Write out data ## write out data pdf_list_df %&gt;% dplyr::select(-c( file_full_path, metashape_quality , metashape_depthmap_filtering, quality_filtering , pdf_title )) %&gt;% dplyr::relocate( c( depth_maps_generation_quality, depth_maps_generation_filtering_mode , total_sparse_point_cloud_processing_time_mins , total_dense_point_cloud_processing_time_mins , dense_point_cloud_points , dense_cloud_generation_file_size_mb , tidyselect::ends_with(&quot;_error_m&quot;) ) , .after = study_site ) %&gt;% dplyr::arrange( study_site, depth_maps_generation_quality, depth_maps_generation_filtering_mode ) %&gt;% dplyr::mutate( dplyr::across( .cols = c(depth_maps_generation_quality, depth_maps_generation_filtering_mode) , .fns = ~ stringr::str_to_title(.x) ) ) %&gt;% write.csv( file = paste0(rootdir, &quot;/metashape_processing_data.csv&quot;) , row.names = F ) 2.3 Metashape Report Data Exploration 2.3.1 Preliminaries What does the data look like? pdf_list_df %&gt;% dplyr::glimpse() ## Rows: 120 ## Columns: 56 ## $ file_full_path &lt;chr&gt; &quot;C:\\\\Data\\\\usfs\\\\metasha… ## $ study_site &lt;chr&gt; &quot;KAIBAB_HIGH&quot;, &quot;KAIBAB_H… ## $ quality_filtering &lt;chr&gt; &quot;HIGH_AGGRESSIVE&quot;, &quot;HIGH… ## $ metashape_quality &lt;chr&gt; &quot;HIGH&quot;, &quot;HIGH&quot;, &quot;HIGH&quot;, … ## $ metashape_depthmap_filtering &lt;chr&gt; &quot;AGGRESSIVE&quot;, &quot;DISABLED&quot;… ## $ pdf_title &lt;chr&gt; &quot;High_Aggressive&quot;, &quot;High… ## $ number_of_images &lt;dbl&gt; 132, 132, 132, 132, 132,… ## $ flying_altitude_m &lt;dbl&gt; 94.1, 94.1, 94.1, 94.1, … ## $ tie_points &lt;dbl&gt; 109282, 109282, 109282, … ## $ ground_resolution_cm_pix &lt;dbl&gt; 2.26, 2.18, 2.20, 2.19, … ## $ coverage_area_km2 &lt;dbl&gt; 0.0396, 0.0399, 0.0396, … ## $ reprojection_error_pix &lt;dbl&gt; 0.704, 0.704, 0.704, 0.7… ## $ x_error_m &lt;dbl&gt; 1.420520, 1.420520, 1.42… ## $ y_error_m &lt;dbl&gt; 1.029610, 1.029610, 1.02… ## $ z_error_m &lt;dbl&gt; 0.475722, 0.475722, 0.47… ## $ xy_error_m &lt;dbl&gt; 1.754410, 1.754410, 1.75… ## $ total_error_m &lt;dbl&gt; 1.817770, 1.817770, 1.81… ## $ general_cameras &lt;dbl&gt; 132, 132, 132, 132, 132,… ## $ general_aligned_cameras &lt;dbl&gt; 132, 132, 132, 132, 132,… ## $ general_coordinate_system &lt;chr&gt; &quot;w gs 84 (epsg::4326)&quot;, … ## $ general_rotation_angles &lt;chr&gt; &quot;yaw, pitch, roll&quot;, &quot;yaw… ## $ point_cloud_points &lt;chr&gt; &quot;109,282 of 118,009&quot;, &quot;1… ## $ point_cloud_rms_reprojection_error &lt;chr&gt; &quot;0.149663 (0.703638 pix)… ## $ point_cloud_max_reprojection_error &lt;chr&gt; &quot;0.453748 (34.8372 pix)&quot;… ## $ point_cloud_mean_key_point_size &lt;chr&gt; &quot;3.80033 pix&quot;, &quot;3.80033 … ## $ point_cloud_point_colors &lt;chr&gt; &quot;3 bands, uint8&quot;, &quot;3 ban… ## $ point_cloud_key_points &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, … ## $ point_cloud_average_tie_point_multiplicity &lt;chr&gt; &quot;3.56183&quot;, &quot;3.56183&quot;, &quot;3… ## $ alignment_accuracy &lt;chr&gt; &quot;high&quot;, &quot;high&quot;, &quot;high&quot;, … ## $ alignment_generic_preselection &lt;chr&gt; &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;ye… ## $ alignment_reference_preselection &lt;chr&gt; &quot;source&quot;, &quot;source&quot;, &quot;sou… ## $ alignment_key_point_limit &lt;chr&gt; &quot;40,000&quot;, &quot;40,000&quot;, &quot;40,… ## $ alignment_tie_point_limit &lt;chr&gt; &quot;4,000&quot;, &quot;4,000&quot;, &quot;4,000… ## $ alignment_guided_image_matching &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, … ## $ alignment_adaptive_camera_model_fitting &lt;chr&gt; &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;ye… ## $ alignment_matching_time_mins &lt;dbl&gt; 1.116667, 1.116667, 1.11… ## $ alignment_matching_memory_usage_mb &lt;dbl&gt; 375.78, 375.78, 375.78, … ## $ alignment_alignment_time_mins &lt;dbl&gt; 0.4833333, 0.4833333, 0.… ## $ alignment_alignment_memory_usage_mb &lt;dbl&gt; 66.08, 66.08, 66.08, 66.… ## $ alignment_software_version &lt;chr&gt; &quot;1.6.4.10928&quot;, &quot;1.6.4.10… ## $ alignment_file_size_mb &lt;dbl&gt; 9.27, 9.27, 9.27, 9.27, … ## $ depth_maps_count &lt;dbl&gt; 132, 132, 132, 132, 132,… ## $ depth_maps_generation_quality &lt;ord&gt; high, high, high, high, … ## $ depth_maps_generation_filtering_mode &lt;ord&gt; aggressive, disabled, mi… ## $ depth_maps_generation_processing_time_mins &lt;dbl&gt; 15.2500000, 15.8166667, … ## $ depth_maps_generation_memory_usage_mb &lt;dbl&gt; 1930.00, 2020.00, 1870.0… ## $ depth_maps_generation_software_version &lt;chr&gt; &quot;1.6.4.10928&quot;, &quot;1.6.4.10… ## $ depth_maps_generation_file_size_mb &lt;dbl&gt; 611.00, 868.53, 775.67, … ## $ dense_point_cloud_points &lt;dbl&gt; 52974294, 72549206, 6985… ## $ dense_point_cloud_point_colors &lt;chr&gt; &quot;3 bands, uint8&quot;, &quot;3 ban… ## $ dense_cloud_generation_processing_time_mins &lt;dbl&gt; 37.9500000, 42.9000000, … ## $ dense_cloud_generation_memory_usage_mb &lt;dbl&gt; 8140.00, 8170.00, 8100.0… ## $ dense_cloud_generation_software_version &lt;chr&gt; &quot;1.6.4.10928&quot;, &quot;1.6.4.10… ## $ dense_cloud_generation_file_size_mb &lt;dbl&gt; 760.56, 1020.00, 1004.88… ## $ total_dense_point_cloud_processing_time_mins &lt;dbl&gt; 53.200000, 58.716667, 54… ## $ total_sparse_point_cloud_processing_time_mins &lt;dbl&gt; 1.600000, 1.600000, 1.60… Do the processing settings match the file names? pdf_list_df %&gt;% dplyr::mutate( quality_match = toupper(depth_maps_generation_quality) %&gt;% stringr::str_remove_all(&quot;\\\\s&quot;) == toupper(metashape_quality) %&gt;% stringr::str_remove_all(&quot;\\\\s&quot;) , filtering_match = toupper(depth_maps_generation_filtering_mode) %&gt;% stringr::str_remove_all(&quot;\\\\s&quot;) == toupper(metashape_depthmap_filtering) %&gt;% stringr::str_remove_all(&quot;\\\\s&quot;) ) %&gt;% dplyr::count(quality_match, filtering_match) %&gt;% kableExtra::kbl(caption=&quot;Do the processing settings match the file names?&quot;) %&gt;% kableExtra::kable_styling() Table 2.2: Do the processing settings match the file names? quality_match filtering_match n TRUE TRUE 120 How many records are there for each depth map generation quality and depth map filtering mode settings? pdf_list_df %&gt;% dplyr::count(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %&gt;% ggplot(mapping = aes( x = n , y = depth_maps_generation_quality , fill = depth_maps_generation_filtering_mode) ) + geom_col(width = 0.7, alpha = 0.8) + geom_text( mapping = aes( group=depth_maps_generation_filtering_mode ,label = scales::comma(n, accuracy = 1) , fontface = &quot;bold&quot; ) , position = position_stack(vjust = 0.5) , color = &quot;black&quot; ) + scale_fill_viridis_d(option = &quot;plasma&quot;) + scale_x_continuous(breaks = scales::extended_breaks(n=14)) + labs( fill = &quot;Filtering Mode&quot; , y = &quot;Quality&quot; , x = &quot;n&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; ) + guides( fill = guide_legend(reverse = T, override.aes = list(alpha = 0.9)) ) How many records are there for each study site? pdf_list_df %&gt;% dplyr::count(depth_maps_generation_quality, depth_maps_generation_filtering_mode, study_site) %&gt;% ggplot(mapping = aes( x = n , y = depth_maps_generation_quality , fill = depth_maps_generation_filtering_mode) ) + geom_col(width = 0.7, alpha = 0.8) + geom_text( mapping = aes( group=depth_maps_generation_filtering_mode ,label = scales::comma(n, accuracy = 1) , fontface = &quot;bold&quot; ) , position = position_stack(vjust = 0.5) , color = &quot;black&quot; ) + facet_wrap(facets = vars(study_site), ncol = 2) + scale_fill_viridis_d(option = &quot;plasma&quot;) + labs( fill = &quot;Filtering Mode&quot; , y = &quot;Quality&quot; , x = &quot;n&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; , strip.text = element_text(color = &quot;black&quot;, face = &quot;bold&quot;) ) + guides( fill = guide_legend(reverse = T, override.aes = list(alpha = 0.9)) ) 2.3.2 Metashape Processing Time Summary Processing time by depth map generation quality and depth map filtering mode pdf_list_df %&gt;% ggplot( mapping = aes( x = depth_maps_generation_quality , y = total_dense_point_cloud_processing_time_mins , color = depth_maps_generation_filtering_mode , fill = depth_maps_generation_filtering_mode ) ) + geom_boxplot(alpha = 0.6) + scale_color_viridis_d(option = &quot;plasma&quot;) + scale_fill_viridis_d(option = &quot;plasma&quot;) + scale_y_log10( labels = scales::comma_format(suffix = &quot; mins&quot;, accuracy = 1) , breaks = scales::breaks_log(n = 8) ) + labs( color = &quot;Filtering Mode&quot; , fill = &quot;Filtering Mode&quot; , y = &quot;Dense Cloud + Depth Map Generation Time &quot; , x = &quot;Quality&quot; , title = &quot;Agisoft Metashape processing time by depth map generation quality and filtering mode&quot; , caption = &quot;*Note the log scale on the y-axis&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; ) + guides( color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9)) ) Why is there such a great spread and left skew for the high and ultra high quality? pdf_list_df %&gt;% ggplot( mapping = aes( y = total_dense_point_cloud_processing_time_mins , x = depth_maps_generation_quality , color = depth_maps_generation_filtering_mode ) ) + geom_point(size = 3, alpha = 0.8) + facet_grid( cols = vars(study_site) , labeller = label_wrap_gen(width = 35, multi_line = TRUE) ) + scale_color_viridis_d(option = &quot;plasma&quot;) + scale_y_log10( labels = scales::comma_format(suffix = &quot; mins&quot;, accuracy = 1) , breaks = scales::breaks_log(n = 8) ) + labs( color = &quot;Filtering Mode&quot; , fill = &quot;Filtering Mode&quot; , y = &quot;Dense Cloud + Depth Map Generation Time &quot; , x = &quot;Quality&quot; , title = &quot;Agisoft Metashape processing time by depth map generation quality and filtering mode&quot; , subtitle = &quot;by Study Site&quot; , caption = &quot;*Note the log scale on the y-axis&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; , strip.text = element_text(color = &quot;black&quot;, face = &quot;bold&quot;) , axis.text.x = element_text(angle = 90) # , strip.text.y.left = element_text(angle = 0) # , strip.placement = &quot;outside&quot; ) + guides( color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9)) ) The study sites “Kaibab_High” and “WA85_02” have faster processing times than the other four sites across all quality settings. 2.3.3 Flight and Sparse Cloud Metrics How do the UAS flight settings and sparse cloud generation parameters differ across sites? pdf_list_df %&gt;% dplyr::filter(quality_filtering==&quot;ULTRAHIGH_MILD&quot;) %&gt;% dplyr::select( study_site , number_of_images , tie_points , ground_resolution_cm_pix , flying_altitude_m , coverage_area_km2 , tidyselect::contains(&quot;_error_m&quot;) ) %&gt;% tidyr::pivot_longer( cols = -c(study_site), names_to = &quot;metric&quot;, values_to = &quot;val&quot; ) %&gt;% ggplot( mapping = aes( x = val , y = study_site , fill = metric ) ) + geom_col(width = 0.7, alpha = 0.8) + facet_wrap(facets = vars(metric), ncol = 2, scales = &quot;free_x&quot;) + scale_fill_viridis_d(option = &quot;cividis&quot;) + labs( y = &quot;&quot; , x = &quot;&quot; , title = &quot;Different processing metrics by study site&quot; , subtitle = &quot;`Ultra High` quality and `Mild` filtering used where applicable (error, resolution, coverage)&quot; ) + theme_light() + theme( legend.position = &quot;none&quot; , strip.text = element_text(color = &quot;black&quot;, face = &quot;bold&quot;) ) The study sites “Kaibab_High” and “WA85_02” have smaller coverage areas, fewer images, and higher x error values than the other four sites. 2.3.4 Metashape Dense Point Cloud Summary Dense point cloud number of points by depth map generation quality and depth map filtering mode pdf_list_df %&gt;% ggplot( mapping = aes( x = depth_maps_generation_quality , y = dense_point_cloud_points , color = depth_maps_generation_filtering_mode , fill = depth_maps_generation_filtering_mode ) ) + geom_boxplot(alpha = 0.6) + scale_color_viridis_d(option = &quot;plasma&quot;) + scale_fill_viridis_d(option = &quot;plasma&quot;) + scale_y_log10( labels = scales::comma_format(suffix = &quot; M&quot;, scale = 1e-6, accuracy = 1) , breaks = scales::breaks_log(n = 6) ) + labs( color = &quot;Filtering Mode&quot; , fill = &quot;Filtering Mode&quot; , y = &quot;Dense Point Cloud # Points&quot; , x = &quot;Quality&quot; , title = &quot;Dense point cloud number of points by depth map generation quality and filtering mode&quot; , caption = &quot;*Note the log scale on the y-axis&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; ) + guides( color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9)) ) Notice there are some outlier study sites in the number of dense cloud points pdf_list_df %&gt;% ggplot( mapping = aes( y = dense_point_cloud_points , x = depth_maps_generation_quality , color = depth_maps_generation_filtering_mode ) ) + geom_point(size = 3, alpha = 0.8) + facet_grid( cols = vars(study_site) , labeller = label_wrap_gen(width = 35, multi_line = TRUE) ) + scale_color_viridis_d(option = &quot;plasma&quot;) + scale_y_log10( labels = scales::comma_format(suffix = &quot; M&quot;, scale = 1e-6, accuracy = 1) , breaks = scales::breaks_log(n = 6) ) + labs( color = &quot;Filtering Mode&quot; , y = &quot;Dense Point Cloud # Points&quot; , x = &quot;Quality&quot; , title = &quot;Dense point cloud number of points by depth map generation quality and filtering mode&quot; , subtitle = &quot;by Study Site&quot; , caption = &quot;*Note the log scale on the y-axis&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; , strip.text = element_text(color = &quot;black&quot;, face = &quot;bold&quot;) , axis.text.x = element_text(angle = 90) ) + guides( color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9)) ) The study site “Kaibab_Low” has fewer dense cloud points than the other five sites for all filtering modes in the “ultra high” and “high” quality settings. The study site “WA85_02” has more dense cloud points than the other five sites for all filtering modes in the “ultra high” quality setting but similar point numbers for the other processing settings. "],["ptcld_analysis.html", "Section 3 R Point Cloud Processing 3.1 Processing Time Summary 3.2 Processing Time vs # Points 3.3 Processing Section Timing", " Section 3 R Point Cloud Processing After running the UAS point cloud processing script in R…the processing tracking data file is used to compare summary statistics on point cloud processing times. ptcld_processing_data = readr::read_csv(file = &quot;../data/ptcld_processing_tracking_data.csv&quot;) %&gt;% dplyr::rename( depth_maps_generation_quality = processing_attribute1 , depth_maps_generation_filtering_mode = processing_attribute2 ) %&gt;% dplyr::mutate( depth_maps_generation_quality = factor( depth_maps_generation_quality %&gt;% tolower() %&gt;% stringr::str_replace_all(&quot;ultrahigh&quot;, &quot;ultra high&quot;) , ordered = TRUE , levels = c( &quot;lowest&quot; , &quot;low&quot; , &quot;medium&quot; , &quot;high&quot; , &quot;ultra high&quot; ) ) %&gt;% forcats::fct_rev() , depth_maps_generation_filtering_mode = factor( depth_maps_generation_filtering_mode %&gt;% tolower() , ordered = TRUE , levels = c( &quot;disabled&quot; , &quot;mild&quot; , &quot;moderate&quot; , &quot;aggressive&quot; ) ) %&gt;% forcats::fct_rev() ) 3.1 Processing Time Summary Total processing time by depth map generation quality and depth map filtering mode ptcld_processing_data %&gt;% ggplot( mapping = aes( x = depth_maps_generation_quality , y = timer_total_time_mins , color = depth_maps_generation_filtering_mode , fill = depth_maps_generation_filtering_mode ) ) + geom_boxplot(alpha = 0.6) + scale_color_viridis_d(option = &quot;plasma&quot;) + scale_fill_viridis_d(option = &quot;plasma&quot;) + scale_y_log10( labels = scales::comma_format(suffix = &quot; mins&quot;, accuracy = 1) , breaks = scales::breaks_log(n = 9) ) + labs( color = &quot;Filtering Mode&quot; , fill = &quot;Filtering Mode&quot; , y = &quot;Point Cloud Total Processing Time&quot; , x = &quot;Quality&quot; , title = bquote( bold(&quot;R&quot;) ~ &quot;point cloud total processing time by depth map generation quality and filtering mode&quot; ) , caption = &quot;*Note the log scale on the y-axis&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; ) + guides( color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9)) ) Notice there are some outlier study sites in the point cloud processing time ptcld_processing_data %&gt;% ggplot( mapping = aes( y = timer_total_time_mins , x = depth_maps_generation_quality , color = depth_maps_generation_filtering_mode ) ) + geom_point(size = 3, alpha = 0.8) + facet_grid( cols = vars(study_site) , labeller = label_wrap_gen(width = 35, multi_line = TRUE) ) + scale_color_viridis_d(option = &quot;plasma&quot;) + scale_y_log10( labels = scales::comma_format(suffix = &quot; mins&quot;, accuracy = 1) , breaks = scales::breaks_log(n = 9) ) + labs( color = &quot;Filtering Mode&quot; , y = &quot;Point Cloud Total Processing Time&quot; , x = &quot;Quality&quot; , title = bquote( bold(&quot;R&quot;) ~ &quot;point cloud total processing time by depth map generation quality and filtering mode&quot; ) , subtitle = &quot;by Study Site&quot; , caption = &quot;*Note the log scale on the y-axis&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; , strip.text = element_text(color = &quot;black&quot;, face = &quot;bold&quot;) , axis.text.x = element_text(angle = 90) ) + guides( color = guide_legend(override.aes = list(shape = 15, size = 6, alpha = 0.9)) ) 3.2 Processing Time vs # Points ptcld_processing_data %&gt;% ggplot( mapping = aes( x = number_of_points , y = timer_total_time_mins ) ) + geom_point(alpha = 0.7, color = &quot;navy&quot;) + scale_y_log10( labels = scales::comma_format(suffix = &quot; mins&quot;, accuracy = 1) , breaks = scales::breaks_log(n = 9) ) + scale_x_log10( labels = scales::comma_format(suffix = &quot; M&quot;, scale = 1e-6, accuracy = 1) , breaks = scales::breaks_log(n = 6) ) + labs( y = &quot;Point Cloud Total Processing Time&quot; , x = &quot;Dense Point Cloud # Points&quot; , title = bquote( bold(&quot;R&quot;) ~ &quot;point cloud total processing time versus dense point cloud number of points&quot; ) , caption = &quot;*Note the log scale on both axes&quot; ) + theme_light() 3.3 Processing Section Timing ptcld_processing_data %&gt;% dplyr::select( depth_maps_generation_quality , tidyselect::ends_with(&quot;_mins&quot;) ) %&gt;% dplyr::select(-c(timer_total_time_mins)) %&gt;% tidyr::pivot_longer( cols = -c(depth_maps_generation_quality) , names_to = &quot;section&quot; , values_to = &quot;mins&quot; ) %&gt;% # dplyr::count(depth_maps_generation_quality, section) dplyr::group_by(depth_maps_generation_quality, section) %&gt;% dplyr::summarise(med_mins = median(mins)) %&gt;% dplyr::group_by(depth_maps_generation_quality) %&gt;% dplyr::mutate( total_mins = sum(med_mins) , pct_mins = med_mins/total_mins ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate( section = section %&gt;% stringr::str_remove_all(&quot;timer_&quot;) %&gt;% stringr::str_remove_all(&quot;_time_mins&quot;) %&gt;% factor( ordered = T , levels = c( &quot;tile&quot; , &quot;denoise&quot; , &quot;classify&quot; , &quot;dtm&quot; , &quot;normalize&quot; , &quot;chm&quot; , &quot;treels&quot; , &quot;itd&quot; , &quot;estdbh&quot; , &quot;competition&quot; , &quot;silv&quot; ) , labels = c( &quot;Tile&quot; , &quot;Denoise&quot; , &quot;Classify&quot; , &quot;DTM&quot; , &quot;Normalize&quot; , &quot;CHM&quot; , &quot;TreeLS SfM DBH&quot; , &quot;CHM I.T.D.&quot; , &quot;Local DBH Est.&quot; , &quot;Tree Competition&quot; , &quot;Silvicultural Metrics&quot; ) ) %&gt;% forcats::fct_rev() ) %&gt;% ggplot( mapping = aes(x = pct_mins, y = depth_maps_generation_quality, fill=section, group=section) ) + geom_col( width = 0.7, alpha=0.8 ) + geom_text( mapping = aes( label = scales::percent(ifelse(pct_mins&gt;=0.06,pct_mins,NA), accuracy = 1) , fontface = &quot;bold&quot; ) , position = position_stack(vjust = 0.5) , color = &quot;black&quot;, size = 4 ) + scale_fill_viridis_d(option = &quot;turbo&quot;) + scale_x_continuous(labels = scales::percent_format()) + labs( fill = &quot;R script\\nsection&quot; , y = &quot;Metashape depth map quality&quot; , x = &quot;% Point Cloud Total Processing Time&quot; , title = bquote( bold(&quot;R&quot;) ~ &quot;point cloud total processing time by Metashape depth map generation quality and R script section&quot; ) , subtitle = &quot;Median across study site and Metashape depth map filtering mode &quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; , legend.title = element_text(size=7) , axis.title.x = element_text(size=10, face = &quot;bold&quot;) , axis.title.y = element_text(size = 8) , axis.text.x = element_blank() , axis.text.y = element_text(color = &quot;black&quot;,size=10, face = &quot;bold&quot;) , axis.ticks.x = element_blank() ) + guides( fill = guide_legend(nrow = 3, byrow = T, reverse = T, override.aes = list(alpha = 0.9)) ) "],["stats_processing_time.html", "Section 4 Statistical Analysis: Processing Time 4.1 One Nominal Predictor 4.2 Two Nominal Predictors", " Section 4 Statistical Analysis: Processing Time In this section, we’ll evaluate the influence of the processing parameters on point cloud processing time. This data was described in this section. The objective of this study is to determine the influence of different structure from motion (SfM) software (e.g. Agisoft Metashap, OpenDroneMap, Pix4D) and processing parameters on processing time needed to create the data required for quantifying forest structure from UAS imagery. The data required includes: i) SfM-derived point cloud(s) in .laz or .las format, and ii) data extracted from these point clouds such as canopy height models (CHM), tree locations, and tree measurments (height and diameter). All of the predictor variables of interest in this study are categorical (i.e. factor or nominal) while the predicted variables are metric and include processing time (continuous &gt; 0) and F-score (ranges from 0-1). This type of statistical analysis is described in the second edition of Kruschke’s Doing Bayesian data analysis (2015): This chapter considers data structures that consist of a metric predicted variable and two (or more) nominal predictors….Data structures of the type considered in this chapter are often encountered in real research. For example, we might want to predict monetary income from political party affiliation and religious affiliation, or we might want to predict galvanic skin response to different combinations of categories of visual stimulus and categories of auditory stimulus. As mentioned in the previous chapter, this type of data structure can arise from experiments or from observational studies. In experiments, the researcher assigns the categories (at random) to the experimental subjects. In observational studies, both the nominal predictor values and the metric predicted value are generated by processes outside the direct control of the researcher. The traditional treatment of this sort of data structure is called multifactor analysis of variance (ANOVA). Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups. Kruschke (2015, pp.583–584) The following analysis will expand the traditional mixed ANOVA approach following the methods outlined by Kassambara in the Comparing Multiple Means in R online course to build a Bayesian approach based on Kruschke (2015). This analysis was greatly enhanced by A. Solomon Kurz’s ebook supplement to Kruschke (2015). 4.1 One Nominal Predictor We’ll start by exploring the influence of the depth map generation quality parameter on the point cloud processing time. 4.1.1 Summary Statistics Summary statistics by group: ptcld_processing_data %&gt;% dplyr::group_by(depth_maps_generation_quality) %&gt;% dplyr::summarise( mean_processing_mins = mean(timer_total_time_mins, na.rm = T) # , med_processing_mins = median(timer_total_time_mins, na.rm = T) , sd_processing_mins = sd(timer_total_time_mins, na.rm = T) , n = dplyr::n() ) %&gt;% kableExtra::kbl(digits = 1, caption = &quot;summary statistics: point cloud processing time by depth map quality&quot;) %&gt;% kableExtra::kable_styling() Table 4.1: summary statistics: point cloud processing time by depth map quality depth_maps_generation_quality mean_processing_mins sd_processing_mins n ultra high 228.1 151.5 23 high 37.0 16.5 24 medium 9.1 2.1 24 low 2.3 0.3 24 lowest 0.8 0.1 24 4.1.2 Linear Model We can use a linear model to obtain means by group: lm1_temp = lm( timer_total_time_mins ~ 0 + depth_maps_generation_quality , data = ptcld_processing_data ) # summary lm1_temp %&gt;% broom::tidy() %&gt;% mutate(term = stringr::str_remove_all(term, &quot;depth_maps_generation_quality&quot;)) %&gt;% kableExtra::kbl(digits = 2, caption = &quot;linear model: point cloud processing time by depth map quality&quot;) %&gt;% kableExtra::kable_styling() Table 4.2: linear model: point cloud processing time by depth map quality term estimate std.error statistic p.value ultra high 228.08 13.96 16.33 0.00 high 37.00 13.67 2.71 0.01 medium 9.13 13.67 0.67 0.51 low 2.25 13.67 0.16 0.87 lowest 0.82 13.67 0.06 0.95 and plot these means with 95% confidence interval lm1_temp %&gt;% broom::tidy() %&gt;% dplyr::bind_cols( lm1_temp %&gt;% confint() %&gt;% dplyr::as_tibble() %&gt;% dplyr::rename(lower = 1, upper = 2) ) %&gt;% mutate( term = term %&gt;% stringr::str_remove_all(&quot;depth_maps_generation_quality&quot;) %&gt;% factor( ordered = TRUE , levels = c( &quot;lowest&quot; , &quot;low&quot; , &quot;medium&quot; , &quot;high&quot; , &quot;ultra high&quot; ) ) %&gt;% forcats::fct_rev() ) %&gt;% ggplot( mapping = aes(x = term, y = estimate, fill = term) ) + geom_col() + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, color = &quot;gray66&quot;) + scale_fill_viridis_d(option = &quot;inferno&quot;) + scale_y_continuous(breaks = scales::extended_breaks(n=8)) + labs(x = &quot;depth map quality&quot;, y = &quot;point cloud processing mins.&quot;) + theme_light() + theme(legend.position = &quot;none&quot;, panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 4.1.3 ANOVA Finally, one-way ANOVA to test for differences in group means aov1_temp = aov( timer_total_time_mins ~ 0 + depth_maps_generation_quality , data = ptcld_processing_data ) # summary aov1_temp %&gt;% broom::tidy() %&gt;% kableExtra::kbl(digits = 2, caption = &quot;one-way ANOVA: point cloud processing time by depth map quality&quot;) %&gt;% kableExtra::kable_styling() Table 4.3: one-way ANOVA: point cloud processing time by depth map quality term df sumsq meansq statistic p.value depth_maps_generation_quality 5 1231482.4 246296.49 54.92 0 Residuals 114 511263.4 4484.77 NA NA The sum of squared residuals is the same between the linear model and the ANOVA model # RSS identical( # linear model lm1_temp$residuals %&gt;% dplyr::as_tibble() %&gt;% mutate(value=value^2) %&gt;% dplyr::pull(value) %&gt;% sum() # anova , summary(aov1_temp)[[1]][[&quot;Sum Sq&quot;]][[2]] ) ## [1] TRUE # F value identical( # linear model summary(lm1_temp)$fstatistic[&quot;value&quot;] %&gt;% unname() %&gt;% round(6) # anova , summary(aov1_temp)[[1]][[&quot;F value&quot;]][[1]] %&gt;% unname() %&gt;% round(6) ) ## [1] TRUE we can use the emmeans package to compare and contrast the mean estimates by group using Tukey’s Honest Significant Differences method. em1_temp = emmeans::contrast( emmeans::emmeans( lm1_temp, ~ depth_maps_generation_quality ) , method = &quot;tukey&quot; ) %&gt;% dplyr::as_tibble() %&gt;% dplyr::mutate( upper = estimate+SE , lower = estimate-SE , sig_lvl = dplyr::case_when( p.value &lt;= 0.01 ~ &quot;0.01&quot; , p.value &lt;= 0.05 ~ &quot;0.05&quot; , p.value &lt;= 0.1 ~ &quot;0.10&quot; , T ~ &quot;not significant&quot; ) %&gt;% factor( ordered = T , levels = c( &quot;0.01&quot; , &quot;0.05&quot; , &quot;0.10&quot; , &quot;not significant&quot; ) ) , sorter = contrast %&gt;% stringr::word(1, sep = &quot;-&quot;) %&gt;% stringr::str_squish() %&gt;% factor( ordered = T , levels = levels(ptcld_processing_data$depth_maps_generation_quality) ) , sorter2 = contrast %&gt;% stringr::word(-1, sep = &quot;-&quot;) %&gt;% stringr::str_squish() %&gt;% factor( ordered = T , levels = levels(ptcld_processing_data$depth_maps_generation_quality) ) , contrast = contrast %&gt;% forcats::fct_reorder( paste0(as.numeric(sorter), as.numeric(sorter2)) %&gt;% as.numeric() ) ) # plot em1_temp %&gt;% # plot ggplot(mapping = aes(y = contrast)) + geom_linerange( mapping = aes(xmin = lower, xmax = upper, color = sig_lvl) , size = 5 , alpha = 0.8 ) + geom_point(mapping = aes(x = estimate)) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray44&quot;) + scale_color_grey() + scale_x_continuous(breaks = scales::extended_breaks(n=8)) + labs( y = &quot;depth map quality&quot; , x = &quot;constrast (mins.)&quot; , subtitle = &quot;Tukey test of mean group constrasts&quot; , color = &quot;sig. level&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; ) and view the contrasts in a table em1_temp %&gt;% dplyr::arrange(desc(contrast)) %&gt;% dplyr::select(contrast, estimate, lower, upper, p.value) %&gt;% kableExtra::kbl( digits = 2, caption = &quot;Tukey&#39;s HSD: depth map quality mean processing time constrasts&quot; ) %&gt;% kableExtra::kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;7in&quot;, height = &quot;6in&quot;) Table 4.4: Tukey’s HSD: depth map quality mean processing time constrasts contrast estimate lower upper p.value low - lowest 1.43 -17.90 20.76 1.00 medium - lowest 8.31 -11.02 27.65 0.99 medium - low 6.88 -12.45 26.22 1.00 high - lowest 36.18 16.85 55.52 0.34 high - low 34.75 15.42 54.09 0.38 high - medium 27.87 8.54 47.20 0.60 ultra high - lowest 227.26 207.72 246.80 0.00 ultra high - low 225.83 206.29 245.37 0.00 ultra high - medium 218.95 199.41 238.49 0.00 ultra high - high 191.08 171.54 210.62 0.00 4.1.4 Bayesian Kruschke (2015) notes: The terminology, “analysis of variance,” comes from a decomposition of overall data variance into within-group variance and between-group variance (Fisher, 1925). Algebraically, the sum of squared deviations of the scores from their overall mean equals the sum of squared deviations of the scores from their respective group means plus the sum of squared deviations of the group means from the overall mean. In other words, the total variance can be partitioned into within-group variance plus between-group variance. Because one definition of the word “analysis” is separation into constituent parts, the term ANOVA accurately describes the underlying algebra in the traditional methods. That algebraic relation is not used in the hierarchical Bayesian approach presented here. The Bayesian method can estimate component variances, however. Therefore, the Bayesian approach is not ANOVA, but is analogous to ANOVA. (p. 556) and see section 19 from Kurz’s ebook supplement The metric predicted variable with one nominal predictor variable model has the form: \\[\\begin{align*} y_{i} &amp;\\sim {\\sf Normal} \\bigl(\\mu_{i}, \\sigma_{y} \\bigr) \\\\ \\mu_{i} &amp;= \\beta_0 + \\sum_{j=1}^{J} \\beta_{1[j]} x_{1[j]} \\bigl(i\\bigr) \\\\ \\beta_{0} &amp;\\sim {\\sf Normal} (0,10) \\\\ \\beta_{1[j]} &amp;\\sim {\\sf Normal} (0,\\sigma_{\\beta_{1}}) \\\\ \\sigma_{\\beta_{1}} &amp;\\sim {\\sf uniform} (0,100) \\\\ \\sigma_{y} &amp;\\sim {\\sf uniform} (0,100) \\\\ \\end{align*}\\] , where \\(j\\) is the depth map generation quality setting corresponding to observation \\(i\\) to start, we’ll use the default brms::brm prior settings which may not match those described in the model specification above brms1_mod = brms::brm( formula = timer_total_time_mins ~ 1 + (1 | depth_maps_generation_quality) , data = ptcld_processing_data , family = brms::brmsfamily(family = &quot;gaussian&quot;) , iter = 3000, warmup = 1000, chains = 4 , file = paste0(rootdir, &quot;/fits/brms1_mod&quot;) ) check the trace plots for problems with convergence of the Markov chains plot(brms1_mod) check the prior distributions # check priors brms::prior_summary(brms1_mod) %&gt;% kableExtra::kbl() %&gt;% kableExtra::kable_styling() prior class coef group resp dpar nlpar lb ub source student_t(3, 8.2, 11) Intercept default student_t(3, 0, 11) sd 0 default sd depth_maps_generation_quality default sd Intercept depth_maps_generation_quality default student_t(3, 0, 11) sigma 0 default The brms::brm model summary brms1_mod %&gt;% brms::posterior_summary() %&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(var = &quot;parameter&quot;) %&gt;% dplyr::rename_with(tolower) %&gt;% dplyr::filter( stringr::str_starts(parameter, &quot;b_&quot;) | stringr::str_starts(parameter, &quot;r_&quot;) | parameter == &quot;sigma&quot; ) %&gt;% dplyr::mutate( parameter = parameter %&gt;% stringr::str_remove_all(&quot;b_depth_maps_generation_quality&quot;) %&gt;% stringr::str_remove_all(&quot;r_depth_maps_generation_quality&quot;) ) %&gt;% kableExtra::kbl(digits = 2, caption = &quot;Bayesian one nominal predictor: point cloud processing time by depth map quality&quot;) %&gt;% kableExtra::kable_styling() Table 4.5: Bayesian one nominal predictor: point cloud processing time by depth map quality parameter estimate est.error q2.5 q97.5 b_Intercept 15.54 15.65 -9.14 53.41 sigma 66.70 4.49 58.74 76.19 [ultra.high,Intercept] 205.69 21.22 161.01 244.50 [high,Intercept] 20.93 20.62 -22.23 58.58 [medium,Intercept] -5.96 20.20 -48.87 30.74 [low,Intercept] -12.93 20.01 -55.59 23.99 [lowest,Intercept] -14.01 20.12 -56.17 22.71 With the stats::coef function, we can get the group-level summaries in a “non-deflection” metric. In the model, the group means represented by \\(\\beta_{1[j]}\\) are deflections from overall baseline, such that the deflections sum to zero (see Kruschke (2015, p.554)). Summaries of the group-specific deflections are available via the brms::ranef function. stats::coef(brms1_mod) %&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(var = &quot;group&quot;) %&gt;% dplyr::rename_with( .cols = -c(&quot;group&quot;) , .fn = ~ stringr::str_remove_all(.x, &quot;depth_maps_generation_quality.&quot;) ) %&gt;% kableExtra::kbl(digits = 2, caption = &quot;brms::brm model: point cloud processing time by depth map quality&quot;) %&gt;% kableExtra::kable_styling() Table 4.6: brms::brm model: point cloud processing time by depth map quality group Estimate.Intercept Est.Error.Intercept Q2.5.Intercept Q97.5.Intercept ultra high 221.23 14.31 193.46 249.45 high 36.48 13.87 9.37 64.11 medium 9.58 13.30 -16.80 35.64 low 2.61 13.28 -23.33 28.35 lowest 1.53 13.32 -24.21 28.34 We can look at the model noise standard deviation \\(\\sigma_y\\) # extract the posterior draws brms::as_draws_df(brms1_mod) %&gt;% # plot ggplot(aes(x = sigma, y = 0)) + tidybayes::stat_dotsinterval( point_interval = median_hdi, .width = .95 , justification = -0.04 , shape = 21, point_size = 3 , quantiles = 100 ) + scale_y_continuous(NULL, breaks = NULL) + xlab(latex2exp::TeX(&quot;$\\\\sigma_y$&quot;)) + theme_light() plot the posterior distributions of the conditional means with the median processing time and the 95% highest posterior density interval (HDI) ptcld_processing_data %&gt;% dplyr::distinct(depth_maps_generation_quality) %&gt;% tidybayes::add_epred_draws(brms1_mod) %&gt;% dplyr::mutate(value = .epred) %&gt;% # plot ggplot( mapping = aes( x = value, y = depth_maps_generation_quality , fill = depth_maps_generation_quality ) ) + tidybayes::stat_halfeye( point_interval = median_hdi, .width = .95 , interval_color = &quot;gray66&quot; , shape = 21, point_color = &quot;gray66&quot;, point_fill = &quot;black&quot; , justification = -0.01 ) + # tidybayes::stat_dotsinterval( # point_interval = median_hdi, .width = .95 # , shape = 21, point_fill = &quot;gray&quot;, justification = -0.04 # , quantiles = 100 # ) + scale_fill_viridis_d(option = &quot;inferno&quot;) + scale_x_continuous(breaks = scales::extended_breaks(n=8)) + labs( y = &quot;depth map quality&quot;, x = &quot;point cloud processing mins.&quot; , subtitle = &quot;posterior distribution of conditional means with 95% HDI&quot; ) + theme_light() + theme(legend.position = &quot;none&quot;) we can also make pairwise comparisons brms1_mod %&gt;% tidybayes::spread_draws(r_depth_maps_generation_quality[depth_maps_generation_quality]) %&gt;% dplyr::mutate( depth_maps_generation_quality = depth_maps_generation_quality %&gt;% stringr::str_replace_all(&quot;\\\\.&quot;, &quot; &quot;) %&gt;% factor( levels = levels(ptcld_processing_data$depth_maps_generation_quality) , ordered = T ) ) %&gt;% dplyr::rename(value = r_depth_maps_generation_quality) %&gt;% tidybayes::compare_levels(value, by = depth_maps_generation_quality) %&gt;% ggplot(aes(x = value, y = depth_maps_generation_quality)) + tidybayes::stat_halfeye( point_interval = median_hdi, .width = .95 , slab_fill = &quot;gray22&quot;, slab_alpha = 1 , interval_color = &quot;gray66&quot;, point_color = &quot;gray66&quot;, point_fill = &quot;black&quot; , shape = 21 , justification = -0.01 ) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray44&quot;) + scale_x_continuous(breaks = scales::extended_breaks(n=8)) + labs( y = &quot;depth map quality&quot; , x = &quot;constrast (mins.)&quot; , subtitle = &quot;95% HDI of the posterior distribution of conditional mean group constrasts&quot; ) + theme_light() + theme(legend.position = &quot;none&quot;) and summarize these contrasts # # can also use the following as substitute for the &quot;tidybayes::spread_draws&quot; used above to get same result ptcld_processing_data %&gt;% dplyr::distinct(depth_maps_generation_quality) %&gt;% tidybayes::add_epred_draws(brms1_mod) %&gt;% dplyr::mutate(value = .epred) %&gt;% tidybayes::compare_levels(value, by = depth_maps_generation_quality) %&gt;% tidybayes::median_hdi() %&gt;% select(-c(.point,.interval)) %&gt;% kableExtra::kbl(digits = 2, caption = &quot;brms::brm model: 95% HDI of the posterior distribution of conditional mean group constrasts&quot;) %&gt;% kableExtra::kable_styling() Table 4.7: brms::brm model: 95% HDI of the posterior distribution of conditional mean group constrasts depth_maps_generation_quality value .lower .upper .width high - ultra high -184.81 -223.80 -145.53 0.95 low - medium -7.07 -43.55 30.70 0.95 lowest - low -0.94 -38.29 35.85 0.95 medium - high -26.87 -66.14 10.28 0.95 4.2 Two Nominal Predictors Now, we’ll determine the combined influence of the depth map generation quality and the depth map filtering parameters on the point cloud processing time. 4.2.1 Summary Statistics Summary statistics by group: ptcld_processing_data %&gt;% dplyr::group_by(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %&gt;% dplyr::summarise( mean_processing_mins = mean(timer_total_time_mins, na.rm = T) # , med_processing_mins = median(timer_total_time_mins, na.rm = T) , sd_processing_mins = sd(timer_total_time_mins, na.rm = T) , n = dplyr::n() ) %&gt;% kableExtra::kbl( digits = 1 , caption = &quot;summary statistics: point cloud processing time by depth map quality and filtering mode&quot; , col.names = c( &quot;depth map quality&quot; , &quot;filtering mode&quot; , &quot;mean time&quot; , &quot;sd&quot; , &quot;n&quot; ) ) %&gt;% kableExtra::kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;7in&quot;, height = &quot;6in&quot;) Table 4.8: summary statistics: point cloud processing time by depth map quality and filtering mode depth map quality filtering mode mean time sd n ultra high aggressive 126.8 58.3 6 ultra high moderate 168.4 89.3 6 ultra high mild 259.9 198.9 6 ultra high disabled 383.1 101.0 5 high aggressive 26.1 4.9 6 high moderate 29.4 3.5 6 high mild 31.6 4.5 6 high disabled 61.0 16.2 6 medium aggressive 7.4 0.7 6 medium moderate 8.2 0.8 6 medium mild 9.6 2.2 6 medium disabled 11.4 2.1 6 low aggressive 2.0 0.2 6 low moderate 2.2 0.2 6 low mild 2.3 0.2 6 low disabled 2.5 0.3 6 lowest aggressive 0.8 0.1 6 lowest moderate 0.8 0.1 6 lowest mild 0.8 0.1 6 lowest disabled 0.9 0.1 6 4.2.2 Linear Model We can use a linear model to obtain means by group: lm2_temp = lm( timer_total_time_mins ~ 1 + depth_maps_generation_quality + depth_maps_generation_filtering_mode + depth_maps_generation_quality:depth_maps_generation_filtering_mode , data = ptcld_processing_data ) # summary predict( lm2_temp , newdata = ptcld_processing_data %&gt;% dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) , interval = &quot;confidence&quot; ) %&gt;% dplyr::as_tibble() %&gt;% dplyr::bind_cols( ptcld_processing_data %&gt;% dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) ) %&gt;% dplyr::relocate(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %&gt;% dplyr::arrange(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %&gt;% kableExtra::kbl( digits = 1 , caption = &quot;linear model: point cloud processing time by depth map quality and filtering mode&quot; , col.names = c( &quot;depth map quality&quot; , &quot;filtering mode&quot; , &quot;y_hat&quot; , &quot;q2.5&quot; , &quot;q97.5&quot; ) ) %&gt;% kableExtra::kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;7in&quot;, height = &quot;6in&quot;) Table 4.9: linear model: point cloud processing time by depth map quality and filtering mode depth map quality filtering mode y_hat q2.5 q97.5 ultra high aggressive 126.8 82.4 171.2 ultra high moderate 168.4 124.0 212.8 ultra high mild 259.9 215.5 304.3 ultra high disabled 383.1 334.5 431.8 high aggressive 26.1 -18.3 70.4 high moderate 29.4 -15.0 73.8 high mild 31.6 -12.8 76.0 high disabled 61.0 16.6 105.4 medium aggressive 7.4 -37.0 51.7 medium moderate 8.2 -36.2 52.6 medium mild 9.6 -34.8 54.0 medium disabled 11.4 -33.0 55.8 low aggressive 2.0 -42.4 46.4 low moderate 2.2 -42.2 46.6 low mild 2.3 -42.1 46.7 low disabled 2.5 -41.9 46.9 lowest aggressive 0.8 -43.6 45.2 lowest moderate 0.8 -43.6 45.2 lowest mild 0.8 -43.5 45.2 lowest disabled 0.9 -43.5 45.3 and plot these means with 95% confidence interval predict( lm2_temp , newdata = ptcld_processing_data %&gt;% dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) , interval = &quot;confidence&quot; ) %&gt;% dplyr::as_tibble() %&gt;% dplyr::bind_cols( ptcld_processing_data %&gt;% dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) ) %&gt;% ggplot( mapping = aes( y = fit , x = depth_maps_generation_quality , fill = depth_maps_generation_filtering_mode , group = depth_maps_generation_filtering_mode ) ) + geom_col(width = 0.7, position = &quot;dodge&quot;) + geom_errorbar( mapping = aes(ymin = lwr, ymax = upr) , width = 0.2, color = &quot;gray66&quot; , position = position_dodge(width = 0.7) ) + scale_fill_viridis_d(option = &quot;plasma&quot;) + scale_y_continuous(breaks = scales::extended_breaks(n=14)) + labs( fill = &quot;filtering mode&quot; , x = &quot;depth map quality&quot; , y = &quot;point cloud processing mins.&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; ) + guides( fill = guide_legend(override.aes = list(alpha = 0.9)) ) 4.2.3 ANOVA Finally, ANOVA to test for differences in group means aov2_temp = aov( timer_total_time_mins ~ 1 + depth_maps_generation_quality + depth_maps_generation_filtering_mode + depth_maps_generation_quality:depth_maps_generation_filtering_mode , data = ptcld_processing_data ) # summary aov2_temp %&gt;% broom::tidy() %&gt;% kableExtra::kbl(digits = 2, caption = &quot;two-way ANOVA: point cloud processing time by depth map quality and filtering mode&quot;) %&gt;% kableExtra::kable_styling() Table 4.10: two-way ANOVA: point cloud processing time by depth map quality and filtering mode term df sumsq meansq statistic p.value depth_maps_generation_quality 4 884385.62 221096.41 73.63 0 depth_maps_generation_filtering_mode 3 52312.64 17437.55 5.81 0 depth_maps_generation_quality:depth_maps_generation_filtering_mode 12 161688.50 13474.04 4.49 0 Residuals 99 297262.23 3002.65 NA NA We can perform pairwise comparisons of the filtering mode between at each depth map quality level # Pairwise comparisons between group levels ptcld_processing_data %&gt;% group_by(depth_maps_generation_quality) %&gt;% rstatix::pairwise_t_test( timer_total_time_mins ~ depth_maps_generation_filtering_mode , p.adjust.method = &quot;bonferroni&quot; ) %&gt;% dplyr::select(-c(n1,p,p.signif,.y.)) %&gt;% kableExtra::kbl( digits = 1 , caption = &quot;Pairwise comparisons between filtering mode at each depth map quality group&quot; ) %&gt;% kableExtra::kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;7in&quot;, height = &quot;6in&quot;) Table 4.11: Pairwise comparisons between filtering mode at each depth map quality group depth_maps_generation_quality group1 group2 n2 p.adj p.adj.signif ultra high aggressive moderate 6 1.0 ns ultra high aggressive mild 6 0.5 ns ultra high moderate mild 6 1.0 ns ultra high aggressive disabled 5 0.0 ultra high moderate disabled 5 0.1 ns ultra high mild disabled 5 0.7 ns high aggressive moderate 6 1.0 ns high aggressive mild 6 1.0 ns high moderate mild 6 1.0 ns high aggressive disabled 6 0.0 **** high moderate disabled 6 0.0 **** high mild disabled 6 0.0 **** medium aggressive moderate 6 1.0 ns medium aggressive mild 6 0.1 ns medium moderate mild 6 0.8 ns medium aggressive disabled 6 0.0 ** medium moderate disabled 6 0.0 medium mild disabled 6 0.4 ns low aggressive moderate 6 1.0 ns low aggressive mild 6 0.1 ns low moderate mild 6 1.0 ns low aggressive disabled 6 0.0 ** low moderate disabled 6 0.2 ns low mild disabled 6 1.0 ns lowest aggressive moderate 6 1.0 ns lowest aggressive mild 6 0.6 ns lowest moderate mild 6 1.0 ns lowest aggressive disabled 6 0.2 ns lowest moderate disabled 6 1.0 ns lowest mild disabled 6 1.0 ns we can use the emmeans package to perform interaction analysis of the mean estimates using Tukey’s Honest Significant Differences method. ## coming soon 4.2.4 Bayesian Kruschke (2015) describes the Hierarchical Bayesian approach to describe groups of metric data with multiple nominal predictors: This chapter considers data structures that consist of a metric predicted variable and two (or more) nominal predictors….The traditional treatment of this sort of data structure is called multifactor analysis of variance (ANOVA). Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups. (pp. 583–584) and see section 20 from Kurz’s ebook supplement The metric predicted variable with two nominal predictor variables model has the form: \\[\\begin{align*} y_{i} &amp;\\sim {\\sf Normal} \\bigl(\\mu_{i}, \\sigma_{y} \\bigr) \\\\ \\mu_{i} &amp;= \\beta_0 + \\sum_{j} \\beta_{1[j]} x_{1[j]} + \\sum_{k} \\beta_{2[k]} x_{2[k]} + \\sum_{j,k} \\beta_{1\\times2[j,k]} x_{1\\times2[j,k]} \\\\ \\beta_{0} &amp;\\sim {\\sf Normal} (0,100) \\\\ \\beta_{1[j]} &amp;\\sim {\\sf Normal} (0,\\sigma_{\\beta_{1}}) \\\\ \\beta_{2[k]} &amp;\\sim {\\sf Normal} (0,\\sigma_{\\beta_{2}}) \\\\ \\beta_{1\\times2[j,k]} &amp;\\sim {\\sf Normal} (0,\\sigma_{\\beta_{1\\times2}}) \\\\ \\sigma_{\\beta_{1}} &amp;\\sim {\\sf Gamma} (1.28,0.005) \\\\ \\sigma_{\\beta_{2}} &amp;\\sim {\\sf Gamma} (1.28,0.005) \\\\ \\sigma_{\\beta_{1\\times2}} &amp;\\sim {\\sf Gamma} (1.28,0.005) \\\\ \\sigma_{y} &amp;\\sim {\\sf Cauchy} (0,109) \\\\ \\end{align*}\\] , where \\(j\\) is the depth map generation quality setting corresponding to observation \\(i\\) and \\(k\\) is the depth map filtering mode setting corresponding to observation \\(i\\) for this model, we’ll define the priors following Kurz who notes that: The noise standard deviation \\(\\sigma_y\\) is depicted in the prior statement including the argument class = sigma…in order to be weakly informative, we will use the half-Cauchy. Recall that since the brms default is to set the lower bound for any variance parameter to 0, there’s no need to worry about doing so ourselves. So even though the syntax only indicates cauchy, it’s understood to mean Cauchy with a lower bound at zero; since the mean is usually 0, that makes this a half-Cauchy…The tails of the half-Cauchy are sufficiently fat that, in practice, I’ve found it doesn’t matter much what you set the \\(SD\\) of its prior to. # from Kurz: gamma_a_b_from_omega_sigma &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } mean_y_temp &lt;- mean(ptcld_processing_data$timer_total_time_mins) sd_y_temp &lt;- sd(ptcld_processing_data$timer_total_time_mins) omega_temp &lt;- sd_y_temp / 2 sigma_temp &lt;- 2 * sd_y_temp s_r_temp &lt;- gamma_a_b_from_omega_sigma(mode = omega_temp, sd = sigma_temp) stanvars_temp &lt;- brms::stanvar(mean_y_temp, name = &quot;mean_y&quot;) + brms::stanvar(sd_y_temp, name = &quot;sd_y&quot;) + brms::stanvar(s_r_temp$shape, name = &quot;alpha&quot;) + brms::stanvar(s_r_temp$rate, name = &quot;beta&quot;) Now fit the model. brms2_mod = brms::brm( formula = timer_total_time_mins ~ 1 + (1 | depth_maps_generation_quality) + (1 | depth_maps_generation_filtering_mode) + (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode) , data = ptcld_processing_data , family = brms::brmsfamily(family = &quot;gaussian&quot;) , iter = 4000, warmup = 2000, chains = 4 , prior = c( brms::prior(normal(mean_y, sd_y * 5), class = &quot;Intercept&quot;) , brms::prior(gamma(alpha, beta), class = &quot;sd&quot;) , brms::prior(cauchy(0, sd_y), class = &quot;sigma&quot;) ) , stanvars = stanvars_temp , file = paste0(rootdir, &quot;/fits/brms2_mod&quot;) ) check the trace plots for problems with convergence of the Markov chains plot(brms2_mod) check the prior distributions # check priors brms::prior_summary(brms2_mod) %&gt;% kableExtra::kbl() %&gt;% kableExtra::kable_styling() prior class coef group resp dpar nlpar lb ub source normal(mean_y, sd_y * 5) Intercept user gamma(alpha, beta) sd 0 user sd depth_maps_generation_filtering_mode default sd Intercept depth_maps_generation_filtering_mode default sd depth_maps_generation_quality default sd Intercept depth_maps_generation_quality default sd depth_maps_generation_quality:depth_maps_generation_filtering_mode default sd Intercept depth_maps_generation_quality:depth_maps_generation_filtering_mode default cauchy(0, sd_y) sigma 0 user The brms::brm model summary brms2_mod %&gt;% brms::posterior_summary() %&gt;% as.data.frame() %&gt;% tibble::rownames_to_column(var = &quot;parameter&quot;) %&gt;% dplyr::rename_with(tolower) %&gt;% dplyr::filter( stringr::str_starts(parameter, &quot;b_&quot;) | stringr::str_starts(parameter, &quot;r_&quot;) | stringr::str_starts(parameter, &quot;sd_&quot;) | parameter == &quot;sigma&quot; ) %&gt;% dplyr::mutate( parameter = parameter %&gt;% stringr::str_replace_all(&quot;depth_maps_generation_quality&quot;, &quot;quality&quot;) %&gt;% stringr::str_replace_all(&quot;depth_maps_generation_filtering_mode&quot;, &quot;filtering&quot;) ) %&gt;% kableExtra::kbl(digits = 2, caption = &quot;Bayesian two nominal predictors: point cloud processing time by depth map quality and filtering mode&quot;) %&gt;% kableExtra::kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;7in&quot;, height = &quot;6in&quot;) Table 4.12: Bayesian two nominal predictors: point cloud processing time by depth map quality and filtering mode parameter estimate est.error q2.5 q97.5 b_Intercept 58.99 70.77 -80.25 202.63 sd_filtering__Intercept 38.51 34.01 2.51 130.31 sd_quality__Intercept 131.49 64.44 56.04 309.48 sd_quality:filtering__Intercept 47.63 13.18 26.78 78.24 sigma 55.55 4.01 48.34 63.88 r_filtering[aggressive,Intercept] -13.80 30.04 -84.99 41.12 r_filtering[moderate,Intercept] -8.42 29.66 -76.47 49.01 r_filtering[mild,Intercept] 2.12 29.25 -60.19 65.14 r_filtering[disabled,Intercept] 18.55 30.63 -33.59 88.03 r_quality[ultra.high,Intercept] 163.23 69.91 25.31 307.89 r_quality[high,Intercept] -19.79 69.05 -160.24 116.54 r_quality[medium,Intercept] -46.22 69.22 -191.95 90.44 r_quality[low,Intercept] -52.79 69.40 -200.08 82.48 r_quality[lowest,Intercept] -54.37 69.29 -193.02 80.58 r_quality:filtering[high_aggressive,Intercept] 0.45 32.64 -64.38 64.36 r_quality:filtering[high_disabled,Intercept] 2.82 33.06 -62.89 68.29 r_quality:filtering[high_mild,Intercept] -7.77 32.79 -73.82 56.86 r_quality:filtering[high_moderate,Intercept] -0.99 32.36 -66.75 62.73 r_quality:filtering[low_aggressive,Intercept] 7.34 32.63 -57.22 71.69 r_quality:filtering[low_disabled,Intercept] -17.71 33.17 -86.12 47.44 r_quality:filtering[low_mild,Intercept] -5.07 32.11 -68.95 56.74 r_quality:filtering[low_moderate,Intercept] 3.02 32.63 -60.54 68.71 r_quality:filtering[lowest_aggressive,Intercept] 7.48 32.52 -56.42 71.09 r_quality:filtering[lowest_disabled,Intercept] -17.59 33.29 -83.95 47.75 r_quality:filtering[lowest_mild,Intercept] -4.61 32.65 -69.63 59.25 r_quality:filtering[lowest_moderate,Intercept] 3.65 32.59 -60.28 68.37 r_quality:filtering[medium_aggressive,Intercept] 6.34 32.52 -58.65 69.55 r_quality:filtering[medium_disabled,Intercept] -16.01 33.72 -84.87 49.60 r_quality:filtering[medium_mild,Intercept] -4.31 32.31 -71.82 57.47 r_quality:filtering[medium_moderate,Intercept] 2.83 32.06 -62.01 65.12 r_quality:filtering[ultra.high_aggressive,Intercept] -64.01 34.97 -132.63 3.39 r_quality:filtering[ultra.high_disabled,Intercept] 109.18 39.90 39.89 194.24 r_quality:filtering[ultra.high_mild,Intercept] 28.75 34.36 -35.73 101.72 r_quality:filtering[ultra.high_moderate,Intercept] -35.42 33.92 -102.54 32.66 We can look at the model noise standard deviation \\(\\sigma_y\\) # extract the posterior draws brms::as_draws_df(brms2_mod) %&gt;% # plot ggplot(aes(x = sigma, y = 0)) + tidybayes::stat_dotsinterval( point_interval = median_hdi, .width = .95 , justification = -0.04 , shape = 21, point_size = 3 , quantiles = 100 ) + scale_y_continuous(NULL, breaks = NULL) + xlab(latex2exp::TeX(&quot;$\\\\sigma_y$&quot;)) + theme_light() plot the posterior distributions of the conditional means with the median processing time and the 95% highest posterior density interval (HDI) ptcld_processing_data %&gt;% dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %&gt;% tidybayes::add_epred_draws(brms2_mod, allow_new_levels = T) %&gt;% dplyr::rename(value = .epred) %&gt;% dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %&gt;% forcats::fct_rev()) %&gt;% # plot ggplot( mapping = aes( y = value, x = depth_maps_generation_filtering_mode , fill = depth_maps_generation_filtering_mode ) ) + tidybayes::stat_eye( point_interval = median_hdi, .width = .95 , slab_alpha = 0.9 , interval_color = &quot;grey66&quot;, linewidth = 1 , shape = 21, point_color = &quot;grey66&quot;, point_fill = &quot;black&quot;, point_size = 1 ) + scale_fill_viridis_d(option = &quot;plasma&quot;) + scale_y_continuous(breaks = scales::extended_breaks(n=10)) + facet_grid(cols = vars(depth_maps_generation_quality)) + labs( x = &quot;filtering mode&quot;, y = &quot;point cloud processing mins.&quot; , subtitle = &quot;posterior distribution of conditional means with 95% HDI&quot; , fill = &quot;Filtering Mode&quot; ) + theme_light() + theme( legend.position = &quot;none&quot; , legend.direction = &quot;horizontal&quot; , axis.text.x = element_text(angle = 90) , strip.text = element_text(color = &quot;black&quot;, face = &quot;bold&quot;) ) # guides( # fill = guide_legend(reverse = T, override.aes = list(alpha = 1, color = NA, shape = NA, lwd = NA)) # ) we can also make pairwise comparisons ptcld_processing_data %&gt;% dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %&gt;% tidybayes::add_epred_draws(brms2_mod, allow_new_levels = T) %&gt;% dplyr::rename(value = .epred) %&gt;% tidybayes::compare_levels(value, by = depth_maps_generation_quality) %&gt;% ggplot( mapping = aes( x = value, y = depth_maps_generation_quality , fill = depth_maps_generation_filtering_mode ) ) + tidybayes::stat_halfeye( point_interval = median_hdi, .width = .95 , slab_alpha = 0.6 , interval_color = &quot;gray66&quot;, interval_alpha = 0.7 , shape = 21, point_color = &quot;gray66&quot;, point_fill = &quot;black&quot;, point_alpha = 0.7 , justification = -0.01 ) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray44&quot;) + scale_fill_viridis_d(option = &quot;plasma&quot;) + scale_x_continuous(breaks = scales::extended_breaks(n=8)) + # facet_grid(cols = vars(depth_maps_generation_filtering_mode)) + labs( y = &quot;depth map quality&quot; , x = &quot;constrast (mins.)&quot; , subtitle = &quot;95% HDI of the posterior distribution of conditional mean group constrasts&quot; , fill = &quot;Filtering Mode&quot; ) + theme_light() + theme( legend.position = &quot;top&quot; , legend.direction = &quot;horizontal&quot; ) + guides( fill = guide_legend(reverse = T, override.aes = list(alpha = 1, color = NA, shape = NA, lwd = NA)) ) and summarize these contrasts # # can also use the following as substitute for the &quot;tidybayes::spread_draws&quot; used above to get same result ptcld_processing_data %&gt;% dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %&gt;% tidybayes::add_epred_draws(brms2_mod, allow_new_levels = T) %&gt;% dplyr::rename(value = .epred) %&gt;% tidybayes::compare_levels(value, by = depth_maps_generation_quality) %&gt;% tidybayes::median_hdi() %&gt;% select(-c(.point,.interval,.width)) %&gt;% kableExtra::kbl( digits = 1, caption = &quot;brms::brm model: 95% HDI of the posterior distribution of conditional mean group constrasts&quot; , col.names = c( &quot;quality contrast&quot; , &quot;filtering mode&quot; , &quot;med constrast (mins.)&quot; , &quot;lower&quot; , &quot;upper&quot; ) ) %&gt;% kableExtra::kable_styling() %&gt;% kableExtra::scroll_box(width = &quot;7in&quot;, height = &quot;6in&quot;) Table 4.13: brms::brm model: 95% HDI of the posterior distribution of conditional mean group constrasts quality contrast filtering mode med constrast (mins.) lower upper high - ultra high aggressive -119.0 -177.0 -58.4 high - ultra high moderate -148.8 -206.1 -87.9 high - ultra high mild -219.6 -279.4 -164.5 high - ultra high disabled -289.7 -353.9 -225.5 low - medium aggressive -5.3 -61.5 50.4 low - medium moderate -7.2 -63.4 52.1 low - medium mild -7.7 -65.3 51.1 low - medium disabled -8.0 -67.2 48.7 lowest - low aggressive -1.3 -62.1 53.0 lowest - low moderate -0.9 -59.6 55.5 lowest - low mild -0.8 -59.7 55.8 lowest - low disabled -1.7 -57.6 56.8 medium - high aggressive -21.0 -76.4 37.2 medium - high moderate -22.7 -77.8 33.8 medium - high mild -23.0 -81.0 33.6 medium - high disabled -45.6 -105.8 11.5 Kruschke (2015) notes that for the multiple nominal predictors model: In applications with multiple levels of the factors, it is virtually always the case that we are interested in comparing particular levels with each other…. These sorts of comparisons, which involve levels of a single factor and collapse across the other factor(s), are called main effect comparisons or contrasts.(p. 595) first, let’s collapse across the filtering mode to compare the depth map quality setting effect ptcld_processing_data %&gt;% dplyr::distinct(depth_maps_generation_quality) %&gt;% tidybayes::add_epred_draws( brms2_mod # this part is crucial , re_formula = ~ (1 | depth_maps_generation_quality) ) %&gt;% dplyr::rename(value = .epred) %&gt;% # plot ggplot( mapping = aes( x = value, y = depth_maps_generation_quality , fill = depth_maps_generation_quality ) ) + tidybayes::stat_halfeye( point_interval = median_hdi, .width = .95 , interval_color = &quot;gray66&quot; , shape = 21, point_color = &quot;gray66&quot;, point_fill = &quot;black&quot; , justification = -0.01 ) + scale_fill_viridis_d(option = &quot;inferno&quot;) + scale_x_continuous(breaks = scales::extended_breaks(n=8)) + labs( y = &quot;depth map quality&quot;, x = &quot;point cloud processing mins.&quot; , subtitle = &quot;posterior distribution of conditional means with 95% HDI&quot; ) + theme_light() + theme(legend.position = &quot;none&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
